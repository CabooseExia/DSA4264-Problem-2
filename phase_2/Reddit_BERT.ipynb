{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cnac963FTDkz"
   },
   "source": [
    "## Reading in Data & Importing Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odImxn2oUQvy"
   },
   "source": [
    "### Set up Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_pNJL2j4XrG"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/School stuff/Y4 S1/DSA4264'\n",
    "\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv3AqqEuxNTr"
   },
   "source": [
    "### Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCjYa1Ibhh8B"
   },
   "source": [
    "### Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPPhVr2XdDX6"
   },
   "outputs": [],
   "source": [
    "%pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QbZp-ASQUImc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import re,nltk\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from plotly.subplots import make_subplots\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuWIEIDrUWeD"
   },
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "URBmj-xj1OVt"
   },
   "outputs": [],
   "source": [
    "Data1 = pd.read_csv('../data/Data_1.csv', engine = 'python') \n",
    "Data2 = pd.read_csv('../data/Data_2.csv', engine = 'python')\n",
    "Data3 = pd.read_csv('../data/Data_3.csv', engine = 'python')\n",
    "Data4 = pd.read_csv('../data/Data_4.csv', engine = 'python')\n",
    "Data5 = pd.read_csv('../data/Data_5.csv', engine = 'python')\n",
    "Data6 = pd.read_csv('../data/Data_6.csv', engine = 'python')\n",
    "Data7 = pd.read_csv('../data/Data_7.csv', engine = 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8-GVBItZ4F0"
   },
   "source": [
    "### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ynl4TiLPKvar"
   },
   "outputs": [],
   "source": [
    "Data = pd.concat([Data1, Data2, Data3, Data4, Data5, Data6, Data7], ignore_index=True, axis=0)\n",
    "\n",
    "Data = Data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nVqLtj9IbvsY"
   },
   "outputs": [],
   "source": [
    "del Data1, Data2, Data3, Data4, Data5, Data6, Data7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bmtLZXiaygh"
   },
   "source": [
    "### Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YLB85kyqa0lv"
   },
   "outputs": [],
   "source": [
    "Data['timestamp']= pd.to_datetime(Data['timestamp'])\n",
    "Data['post_timestamp']= pd.to_datetime(Data['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6nQFh3FbcEc"
   },
   "source": [
    "## BERT Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU4xtuBHd_ml"
   },
   "source": [
    "Repeat for r/Singapore, r/SingaporeRaw, r/SingaporeHappenings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzAAwbUwbbxw"
   },
   "outputs": [],
   "source": [
    "# Exclude specific post_titles for r/Singapore\n",
    "exclude_condition = Data['post_title'].str.startswith('/r/singapore random discussion and small questions thread for').fillna(False)\n",
    "Data_filtered = Data[~((Data['subreddit'] == 'r/Singapore') & exclude_condition)]\n",
    "\n",
    "# Filter for subreddit == r/SingaporeHappenings\n",
    "singapore_posts = Data_filtered[Data_filtered['subreddit'] == 'r/SingaporeHappenings']\n",
    "\n",
    "# Get unique post_title \n",
    "unique_post_titles = singapore_posts['post_title'].dropna().unique()\n",
    "\n",
    "# Initialize BERTopic\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model, language=\"english\")\n",
    "\n",
    "# Fit BERTopic on the unique post titles\n",
    "topics, probabilities = topic_model.fit_transform(unique_post_titles)\n",
    "\n",
    "# Create a DataFrame with the results for the unique post titles\n",
    "topic_df = pd.DataFrame({\n",
    "    'post_title': unique_post_titles,\n",
    "    'topic': topics,\n",
    "    'topic_probability': probabilities\n",
    "})\n",
    "\n",
    "\n",
    "def get_topic_words(topic):\n",
    "    return ' '.join([word for word, _ in topic_model.get_topic(topic, full=True)['Main']][:4])\n",
    "\n",
    "topic_df['topic_words'] = topic_df['topic'].apply(get_topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wxl_sO3rd3p0"
   },
   "outputs": [],
   "source": [
    "topic_df.to_csv('../data/Data.csv', encoding = 'utf-8-sig')\n",
    "files.download('../data/Data.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
