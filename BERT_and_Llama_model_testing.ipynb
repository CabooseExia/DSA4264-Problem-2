{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Setup\n",
    "\n",
    "Follow these steps to set up the necessary files and structure for the project.\n",
    "\n",
    "## Folder Structure and Files\n",
    "\n",
    "1. **Create a `data` Folder**: In the main project directory, create a folder named `data` and place the file `labeled_data_2.csv` inside it.\n",
    "\n",
    "2. **Create a `.env` File**: In the main project directory, create a file named `.env`.\n",
    "\n",
    "3. **Add Your Hugging Face API Key**:\n",
    "   - Open the `.env` file and add the following line:\n",
    "   \n",
    "     ```plaintext\n",
    "     HUGGINGFACE_API_KEY=your_api_key_here\n",
    "     ```\n",
    "\n",
    "   - Replace `your_api_key_here` with your actual Hugging Face API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\caboo\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Device in use: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# NLP and Transformers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# API and Hugging Face Integration\n",
    "import requests\n",
    "from huggingface_hub import login\n",
    "\n",
    "# AI APIs\n",
    "import google.generativeai as genai\n",
    "from googleapiclient import discovery\n",
    "from openai import OpenAI\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# huggingface API key\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "login(token=hf_api_key)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f'Device in use: {device_name}')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Device in use: CPU')\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        timestamp  \\\n",
      "0     Expensive eh now that Uglyfoods closed down :(   30/1/2023 1:04   \n",
      "1                How dare you.. wan go lim kopi ah??   4/5/2022 18:57   \n",
      "2  Yeah the governments can politick all they wan...  28/6/2022 13:44   \n",
      "3               Hijacks event, then complains. Wild.   12/7/2022 7:29   \n",
      "4  Hate to break it to you. But once someone accu...   23/8/2023 2:08   \n",
      "\n",
      "              username                                               link  \\\n",
      "0      MangoDangoLango  /r/singapore/comments/10nqt5h/rsingapore_rando...   \n",
      "1               900122  /r/SingaporeRaw/comments/ui0rmg/dont_take_offe...   \n",
      "2  DisillusionedSinkie  /r/singapore/comments/vmb197/malaysias_top_tal...   \n",
      "3            nehjipain  /r/singapore/comments/vx42x1/nus_student_tried...   \n",
      "4          KeenStudent  /r/singapore/comments/15ybdme/sorry_doesnt_cut...   \n",
      "\n",
      "      link_id   parent_id       id subreddit_id  \\\n",
      "0  t3_10nqt5h  t1_j6dwxo8  j6fuv4x     t5_2qh8c   \n",
      "1   t3_ui0rmg  t1_i79scst  i7bsqea     t5_xnx04   \n",
      "2   t3_vmb197  t1_ie1fiyf  ie1ycm0     t5_2qh8c   \n",
      "3   t3_vx42x1  t1_iftsm0q  iftwbcz     t5_2qh8c   \n",
      "4  t3_15ybdme  t1_jxc6g7p  jxcxjd6     t5_2qh8c   \n",
      "\n",
      "                                          moderation  toxic  hateful  combined  \n",
      "0  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "1  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "2  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "3  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "4  {'banned_at_utc': None, 'mod_reason_by': None,...  False    False     False  \n",
      "(300, 12)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "###   VALIDATION DATASET   ###\n",
    "df = pd.read_csv('data/labeled_data_2.csv')\n",
    "df['combined'] = df['hateful'] | df['toxic']\n",
    "print(df.head())\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df\n",
    "\n",
    "### removing deleted or removed text ###\n",
    "df_normalized = df_normalized[df_normalized['text'] != '[deleted]']\n",
    "df_normalized = df_normalized[df_normalized['text'] != '[removed]']\n",
    "df_normalized = df_normalized.dropna(subset=['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Finding the best labeller\n",
    "https://huggingface.co/sileod/deberta-v3-base-tasksource-toxicity\n",
    "\n",
    "\n",
    "Originally, we planned to separate toxic and hate content to allow for a more nuanced analysis. However, after evaluating our approach, we realized that both types of content would lead to the same final recommendations. To streamline our process, we decided to combine toxic and hate categories instead of retaining them separately. This way, we focused our analysis on the combined data to drive straightforward, actionable insights. <br>\n",
    "\n",
    "After testing, as sileod/deberta-v3-base-tasksource-toxicity has the best high f1 score and a takes a relatively low time to label the text data, we decided to use it. <br>\n",
    "| Model                                         | Best Toxic F1 Score | Toxic Threshold | Best Hate F1 Score | Hate Threshold | Combined Best F1 Score | Combined Threshold | Time Taken |\n",
    "|-----------------------------------------------|----------------------|-----------------|--------------------|----------------|------------------------|--------------------|------------|\n",
    "| sileod/deberta-v3-base-tasksource-toxicity    | 0.547368            | 0.01            | 0.573034           | 0.04          | 0.675079               | 0.01               | 12s        |\n",
    "| unitary/toxic-bert                            | 0.543689            | 0.00            | 0.513889           | 0.40          | 0.648649               | 0.00               | 4s         |\n",
    "| GroNLP/hateBERT                               | 0.554455            | 0.40            | 0.397849           | 0.38          | 0.651584               | 0.38               | 4s         |\n",
    "| textdetox/xlmr-large-toxicity-classifier      | 0.540146            | 0.00            | 0.493671           | 0.05          | 0.645598               | 0.00               | 4s         |\n",
    "| facebook/roberta-hate-speech-dynabench-r4-target | 0.540146         | 0.00            | 0.422360           | 0.04          | 0.645598               | 0.00               | 4s         |\n",
    "| cointegrated/rubert-tiny-toxicity             | 0.540146            | 0.00            | 0.429268           | 0.04          | 0.645598               | 0.00               | 1s         |\n",
    "| badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification | 0.540146 | 0.00 | 0.391421 | 0.00 | 0.645598 | 0.00 | 2s         |\n",
    "| citizenlab/distilbert-base-multilingual-cased-toxicity | 0.540146 | 0.00 | 0.48062  | 0.57 | 0.645598 | 0.00 | 3s         |\n",
    "| GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1 | 0.543689 | 0.00 | 0.395722 | 0.00 | 0.648649 | 0.00 | 3s         |\n",
    "| Hate-speech-CNERG/dehatebert-mono-english     | 0.543689            | 0.00            | 0.476190           | 0.08          | 0.648649               | 0.00               | 4s         |\n",
    "| cardiffnlp/twitter-roberta-base-hate          | 0.540146            | 0.00            | 0.423077           | 0.04          | 0.645598               | 0.00               | 4s         |\n",
    "| Hate-speech-CNERG/bert-base-uncased-hatexplain | 0.571429           | 0.04            | 0.444444           | 0.04          | 0.658824               | 0.04               | 6s         |\n",
    "| mrm8488/distilroberta-finetuned-tweets-hate-speech | 0.556122          | 0.04            | 0.391421           | 0.00          | 0.646226               | 0.04               | 2s         |\n",
    "| meta-llama/Llama-3.2-1B-Instruct              | 0.0992908           | NaN             | 0.211382           | NaN           | 0.364341               | NaN                | 30s        |\n",
    "| meta-llama/Llama-3.2-3B-Instruct              | 0.493023            | NaN             | 0.390244           | NaN           | 0.534483               | NaN                | 14min      |\n",
    "| aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct | 0.514286         | NaN             | 0.324786           | NaN           | 0.517857               | NaN                | 53min      |\n",
    "\n",
    "All the test results will be below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Testing Bert models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 sileod/deberta-v3-base-tasksource-toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::   3%|▎         | 9/300 [00:00<00:11, 24.58it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Classifying:: 100%|██████████| 300/300 [00:12<00:00, 23.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.01, Best F1 Score: 0.5473684210526316\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.5730337078651685\n",
      "Best Threshold for combined: 0.01, Best F1 Score: 0.6750788643533123\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     173\n",
      "False    127\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    196\n",
      "True     104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     173\n",
      "False    127\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'sileod/deberta-v3-base-tasksource-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 unitary/toxic-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Classifying:: 100%|██████████| 300/300 [00:05<00:00, 58.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.4, Best F1 Score: 0.5138888888888888\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    230\n",
      "True      70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'unitary/toxic-bert'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 GroNLP/hateBERT (loves to fluctuate .-.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:05<00:00, 57.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.48, Best F1 Score: 0.5521126760563381\n",
      "Best Threshold for hateful: 0.44, Best F1 Score: 0.3967391304347826\n",
      "Best Threshold for combined: 0.43, Best F1 Score: 0.65\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     243\n",
      "False     57\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     294\n",
      "False      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     296\n",
      "False      4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'GroNLP/hateBERT'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_0':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 textdetox/xlmr-large-toxicity-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  38%|███▊      | 115/300 [00:02<00:03, 56.80it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  43%|████▎     | 128/300 [00:02<00:03, 54.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (547) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 547].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:05<00:00, 54.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.05, Best F1 Score: 0.4936708860759494\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    216\n",
      "True      84\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'textdetox/xlmr-large-toxicity-classifier'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 facebook/roberta-hate-speech-dynabench-r4-target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  39%|███▉      | 118/300 [00:02<00:03, 54.89it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  44%|████▎     | 131/300 [00:02<00:02, 57.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:05<00:00, 54.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.422360248447205\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    213\n",
      "True      87\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'facebook/roberta-hate-speech-dynabench-r4-target'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 cointegrated/rubert-tiny-toxicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  38%|███▊      | 115/300 [00:00<00:01, 122.63it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  47%|████▋     | 142/300 [00:01<00:01, 121.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The size of tensor a (553) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:02<00:00, 121.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 1.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.96, Best F1 Score: 0.4292682926829268\n",
      "Best Threshold for combined: 1.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    169\n",
      "True     131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'cointegrated/rubert-tiny-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'non-toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x < threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] < best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  39%|███▊      | 116/300 [00:01<00:01, 99.77it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  42%|████▏     | 127/300 [00:01<00:01, 99.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:03<00:00, 94.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 1.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 1.0, Best F1 Score: 0.3914209115281501\n",
      "Best Threshold for combined: 1.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'NEITHER':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x < threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] < best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 citizenlab/distilbert-base-multilingual-cased-toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  38%|███▊      | 114/300 [00:01<00:02, 80.38it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  44%|████▎     | 131/300 [00:01<00:02, 76.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The size of tensor a (553) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:03<00:00, 75.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.5700000000000001, Best F1 Score: 0.4806201550387597\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    245\n",
      "True      55\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'citizenlab/distilbert-base-multilingual-cased-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.9 GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1 <br> \n",
    "class_labels = [\"neither\", \"offensive\", \"hate\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 69.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.0, Best F1 Score: 0.39572192513368987\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_2':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.10 Hate-speech-CNERG/dehatebert-mono-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:05<00:00, 55.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.08, Best F1 Score: 0.47619047619047616\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    206\n",
      "True      94\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'Hate-speech-CNERG/dehatebert-mono-english'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'HATE':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.11 cardiffnlp/twitter-roberta-base-hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  43%|████▎     | 129/300 [00:02<00:03, 56.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:05<00:00, 53.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.4230769230769231\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     186\n",
      "False    114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'cardiffnlp/twitter-roberta-base-hate'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.12 Hate-speech-CNERG/bert-base-uncased-hatexplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Classifying:: 100%|██████████| 300/300 [00:07<00:00, 42.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.04, Best F1 Score: 0.5714285714285714\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.4444444444444444\n",
      "Best Threshold for combined: 0.04, Best F1 Score: 0.6588235294117647\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'Hate-speech-CNERG/bert-base-uncased-hatexplain'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate speech':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.13 mrm8488/distilroberta-finetuned-tweets-hate-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  39%|███▊      | 116/300 [00:01<00:02, 91.58it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  45%|████▌     | 136/300 [00:01<00:01, 92.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:03<00:00, 87.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.04, Best F1 Score: 0.5561224489795918\n",
      "Best Threshold for hateful: 0.0, Best F1 Score: 0.3914209115281501\n",
      "Best Threshold for combined: 0.04, Best F1 Score: 0.6462264150943396\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     280\n",
      "False     20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     280\n",
      "False     20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'mrm8488/distilroberta-finetuned-tweets-hate-speech'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_0':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 testing Llama models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\caboo\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model='meta-llama/Llama-3.2-1B-Instruct', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:00<01:08,  4.37it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:00<00:49,  6.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:00<00:36,  8.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:00<00:35,  8.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:00<00:34,  8.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:01<00:44,  6.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:01<00:40,  7.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:01<00:35,  8.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:01<00:35,  8.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:01<00:33,  8.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:01<00:30,  9.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:02<01:04,  4.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:02<01:01,  4.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:02<00:58,  4.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:02<00:52,  5.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:03<00:53,  5.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:03<00:49,  5.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:03<00:44,  6.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:03<00:43,  6.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:03<00:40,  6.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:03<00:37,  7.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [00:03<00:37,  7.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:03<00:35,  7.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:04<00:31,  8.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [00:04<00:29,  9.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [00:04<00:29,  9.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [00:04<00:29,  9.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [00:04<00:29,  8.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [00:04<00:27,  9.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [00:05<00:30,  8.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [00:05<00:30,  8.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [00:05<00:29,  8.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [00:05<00:30,  8.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [00:05<00:29,  8.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [00:05<00:27,  9.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [00:06<00:25,  9.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [00:06<00:25,  9.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [00:06<00:25,  9.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [00:06<00:30,  8.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [00:06<00:30,  8.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [00:06<00:29,  8.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [00:06<00:26,  9.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [00:07<00:24,  9.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [00:07<00:25,  9.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [00:07<00:25,  9.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [00:07<00:24,  9.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [00:07<00:25,  9.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [00:07<00:25,  9.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [00:07<00:25,  9.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [00:08<00:25,  9.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [00:08<00:24,  9.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [00:08<00:23,  9.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [00:08<00:24,  9.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [00:08<00:23,  9.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [00:08<00:24,  9.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [00:09<00:23,  9.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [00:09<00:23,  9.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [00:09<00:23,  9.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [00:09<00:21, 10.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [00:09<00:21, 10.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [00:09<00:21,  9.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [00:10<00:21,  9.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [00:10<00:20, 10.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [00:10<00:20, 10.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [00:10<00:20, 10.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [00:10<00:20, 10.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [00:11<00:20,  9.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [00:11<00:19, 10.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [00:11<00:19, 10.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [00:11<00:18, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [00:11<00:19, 10.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [00:12<00:18, 10.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [00:12<00:17, 10.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [00:12<00:17, 11.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [00:12<00:17, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [00:12<00:17, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [00:12<00:17, 10.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [00:13<00:17, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [00:13<00:17, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [00:13<00:17, 10.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [00:13<00:21,  8.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:14<00:22,  7.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:14<00:20,  8.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:14<00:18,  9.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:14<00:17,  9.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:14<00:17,  9.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▍     | 134/300 [00:15<00:18,  9.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 136/300 [00:15<00:16,  9.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:15<00:16,  9.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 138/300 [00:15<00:16,  9.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 140/300 [00:15<00:15, 10.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 142/300 [00:15<00:15, 10.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 144/300 [00:16<00:14, 10.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▊     | 146/300 [00:16<00:14, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 148/300 [00:16<00:13, 10.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 150/300 [00:16<00:14, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 152/300 [00:16<00:14, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████▏    | 154/300 [00:16<00:13, 10.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 156/300 [00:17<00:13, 10.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 158/300 [00:17<00:13, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 160/300 [00:17<00:12, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 162/300 [00:17<00:12, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▍    | 164/300 [00:17<00:13, 10.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 166/300 [00:18<00:12, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 168/300 [00:18<00:12, 10.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 170/300 [00:18<00:12, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 172/300 [00:18<00:12, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 174/300 [00:18<00:11, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▊    | 176/300 [00:19<00:11, 10.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 178/300 [00:19<00:11, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 180/300 [00:19<00:11, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 182/300 [00:19<00:10, 10.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████▏   | 184/300 [00:19<00:10, 11.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 186/300 [00:19<00:10, 10.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 188/300 [00:20<00:10, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 190/300 [00:20<00:09, 11.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 192/300 [00:20<00:09, 11.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▍   | 194/300 [00:20<00:09, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 196/300 [00:20<00:09, 11.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 198/300 [00:21<00:08, 11.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 200/300 [00:21<00:08, 11.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 202/300 [00:21<00:08, 11.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 204/300 [00:21<00:09,  9.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▊   | 206/300 [00:21<00:08, 10.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 208/300 [00:22<00:09,  9.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 210/300 [00:22<00:08, 10.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 212/300 [00:22<00:08, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████▏  | 214/300 [00:22<00:08, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 216/300 [00:22<00:08, 10.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 218/300 [00:22<00:07, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 220/300 [00:23<00:07, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 222/300 [00:23<00:06, 11.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▍  | 224/300 [00:23<00:06, 11.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 226/300 [00:23<00:06, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 228/300 [00:23<00:06, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 230/300 [00:24<00:06, 10.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 232/300 [00:24<00:06, 10.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 234/300 [00:24<00:06, 10.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▊  | 236/300 [00:24<00:05, 10.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 238/300 [00:24<00:05, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 240/300 [00:24<00:05, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 242/300 [00:25<00:06,  9.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████▏ | 244/300 [00:25<00:05,  9.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 246/300 [00:25<00:05,  9.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 248/300 [00:25<00:05,  9.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 250/300 [00:26<00:04, 10.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 252/300 [00:26<00:05,  9.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▍ | 254/300 [00:26<00:04, 10.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 256/300 [00:26<00:04, 10.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 258/300 [00:26<00:04,  9.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 260/300 [00:27<00:03, 10.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 262/300 [00:27<00:03,  9.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 264/300 [00:27<00:03,  9.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▊ | 266/300 [00:27<00:03,  9.82it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 268/300 [00:27<00:03, 10.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 270/300 [00:28<00:03,  9.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 272/300 [00:28<00:02, 10.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████▏| 274/300 [00:28<00:02, 10.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 276/300 [00:28<00:02,  9.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 278/300 [00:28<00:02,  9.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:28<00:02,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:29<00:01, 10.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:29<00:01, 10.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:29<00:01, 10.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:29<00:01, 10.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:29<00:01, 10.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:30<00:00, 10.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:30<00:00, 10.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:30<00:00,  9.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:30<00:00, 10.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:30<00:00, 10.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:30<00:00,  9.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic or hateful. Please respond with only False (not toxic or hateful) or True (very toxic or hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_combined'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_combined'] = df_normalized['Llama_combined'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:00<00:33,  8.90it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:00<00:27, 10.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:00<00:28, 10.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:00<00:26, 10.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:00<00:25, 11.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:01<00:25, 11.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:01<00:25, 11.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:01<00:38,  7.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:01<00:38,  7.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:01<00:32,  8.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:02<00:36,  7.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:02<00:34,  8.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:02<00:30,  9.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:02<00:27,  9.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:02<00:28,  9.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:02<00:27,  9.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [00:03<00:26, 10.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [00:03<00:26, 10.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [00:03<00:26, 10.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [00:03<00:25, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [00:04<00:29,  9.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [00:04<00:26,  9.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [00:04<00:26,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [00:04<00:24, 10.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [00:04<00:23, 10.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [00:04<00:24, 10.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [00:05<00:24, 10.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [00:05<00:23, 10.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [00:05<00:22, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [00:05<00:22, 11.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [00:05<00:22, 10.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [00:06<00:23, 10.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [00:06<00:22, 10.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [00:06<00:22, 10.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [00:06<00:22, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [00:06<00:21, 10.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [00:06<00:20, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [00:07<00:21, 10.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [00:07<00:21, 10.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [00:07<00:20, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [00:07<00:19, 11.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [00:07<00:19, 11.34it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [00:08<00:20, 10.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [00:08<00:20, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [00:08<00:20, 10.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [00:08<00:19, 10.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [00:08<00:18, 11.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [00:09<00:19, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [00:09<00:18, 11.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [00:09<00:19, 10.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [00:09<00:19, 10.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [00:09<00:18, 10.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [00:09<00:19, 10.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [00:10<00:18, 10.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [00:10<00:18, 10.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [00:10<00:17, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [00:10<00:17, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [00:10<00:16, 11.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [00:11<00:16, 10.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [00:11<00:17, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [00:11<00:16, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [00:11<00:17, 10.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [00:11<00:21,  8.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [00:12<00:19,  9.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:12<00:21,  8.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:12<00:19,  9.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:12<00:17,  9.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:12<00:16, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:13<00:15, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [00:13<00:16, 10.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:13<00:16, 10.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [00:13<00:15, 10.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [00:13<00:15, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [00:14<00:14, 10.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [00:14<00:14, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [00:14<00:14, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [00:14<00:14, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [00:14<00:13, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [00:14<00:13, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [00:15<00:13, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [00:15<00:12, 11.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [00:15<00:12, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [00:15<00:12, 11.11it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [00:15<00:12, 11.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [00:16<00:12, 10.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [00:16<00:12, 10.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [00:16<00:12, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [00:16<00:12, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [00:16<00:12, 10.34it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [00:16<00:11, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [00:17<00:11, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [00:17<00:11, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [00:17<00:10, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [00:17<00:10, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [00:17<00:10, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [00:18<00:10, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [00:18<00:10, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [00:18<00:09, 11.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [00:18<00:09, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [00:18<00:09, 11.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [00:18<00:08, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [00:19<00:08, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [00:19<00:08, 11.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [00:19<00:09,  9.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [00:19<00:09, 10.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [00:19<00:09,  9.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [00:20<00:08, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [00:20<00:08, 10.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [00:20<00:08, 10.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [00:20<00:07, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [00:20<00:07, 10.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [00:21<00:07, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [00:21<00:07, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [00:21<00:06, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [00:21<00:06, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [00:21<00:06, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [00:21<00:06, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [00:22<00:06, 10.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [00:22<00:06, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [00:22<00:05, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [00:22<00:05, 11.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [00:22<00:05, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [00:23<00:05, 10.82it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [00:23<00:05,  9.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [00:23<00:05, 10.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [00:23<00:05, 10.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [00:23<00:04, 10.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [00:24<00:04, 10.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [00:24<00:04,  9.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [00:24<00:04, 10.11it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [00:24<00:04, 10.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [00:24<00:03, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [00:25<00:03, 10.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [00:25<00:03, 10.11it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [00:25<00:03, 10.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [00:25<00:03, 10.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [00:25<00:02, 10.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [00:26<00:02, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [00:26<00:02, 10.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [00:26<00:02,  9.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [00:26<00:02,  9.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:26<00:02, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:26<00:01, 10.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:27<00:01, 11.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:27<00:01, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:27<00:01, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:27<00:01, 10.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:27<00:00, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:28<00:00, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:28<00:00, 10.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:28<00:00, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:28<00:00, 10.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:28<00:00, 10.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic. Please respond with only False (not toxic) or True (very toxic) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_toxic'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_toxic'] = df_normalized['Llama_toxic'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:00<00:31,  9.53it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:00<00:27, 10.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:00<00:28, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:00<00:26, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:00<00:25, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:01<00:26, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:01<00:25, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:01<00:25, 11.11it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:01<00:27, 10.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:01<00:30,  9.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:01<00:29,  9.34it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:02<00:27, 10.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:02<00:26, 10.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:02<00:26, 10.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:02<00:26, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [00:02<00:24, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [00:03<00:26, 10.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [00:03<00:26,  9.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [00:03<00:25, 10.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [00:03<00:28,  9.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [00:03<00:26,  9.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [00:04<00:25,  9.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [00:04<00:25, 10.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [00:04<00:24, 10.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [00:04<00:23, 10.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [00:04<00:23, 10.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [00:05<00:22, 10.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [00:05<00:21, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [00:05<00:21, 11.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [00:05<00:22, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [00:05<00:21, 10.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [00:05<00:21, 11.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [00:06<00:22, 10.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [00:06<00:21, 10.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [00:06<00:20, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [00:06<00:20, 11.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [00:06<00:21, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [00:07<00:21, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [00:07<00:21, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [00:07<00:20, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [00:07<00:20, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [00:07<00:20, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [00:07<00:19, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [00:08<00:19, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [00:08<00:19, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [00:08<00:18, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [00:08<00:19, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [00:08<00:19, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [00:09<00:19, 10.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [00:09<00:18, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [00:09<00:18, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [00:09<00:18, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [00:09<00:17, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [00:09<00:17, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [00:10<00:17, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [00:10<00:17, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [00:10<00:17, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [00:10<00:16, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [00:10<00:17, 10.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [00:11<00:16, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [00:11<00:17, 10.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [00:11<00:21,  8.34it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [00:11<00:19,  9.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:12<00:22,  7.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:12<00:20,  8.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:12<00:18,  9.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:12<00:17,  9.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:12<00:16,  9.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [00:12<00:16,  9.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:13<00:16, 10.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [00:13<00:15, 10.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [00:13<00:15, 10.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [00:13<00:14, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [00:13<00:13, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [00:14<00:13, 11.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [00:14<00:13, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [00:14<00:13, 11.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [00:14<00:12, 11.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [00:14<00:12, 11.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [00:14<00:12, 11.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [00:15<00:12, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [00:15<00:11, 11.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [00:15<00:11, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [00:15<00:11, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [00:15<00:12, 10.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [00:16<00:12, 10.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [00:16<00:12, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [00:16<00:12, 10.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [00:16<00:11, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [00:16<00:11, 10.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [00:16<00:11, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [00:17<00:10, 11.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [00:17<00:10, 11.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [00:17<00:10, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [00:17<00:10, 10.82it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [00:17<00:10, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [00:18<00:10, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [00:18<00:09, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [00:18<00:09, 11.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [00:18<00:09, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [00:18<00:09, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [00:18<00:09, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [00:19<00:10,  9.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 204/300 [00:19<00:09,  9.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▊   | 206/300 [00:19<00:09, 10.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 208/300 [00:19<00:09,  9.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [00:19<00:09,  9.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [00:20<00:08, 10.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [00:20<00:08, 10.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [00:20<00:08, 10.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [00:20<00:07, 10.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [00:20<00:07, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [00:20<00:07, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [00:21<00:07, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [00:21<00:06, 10.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [00:21<00:06, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [00:21<00:06, 10.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [00:21<00:06, 10.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [00:22<00:06, 10.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [00:22<00:05, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [00:22<00:05, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [00:22<00:05, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [00:22<00:05, 10.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [00:23<00:06,  9.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [00:23<00:05,  9.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [00:23<00:05, 10.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [00:23<00:05, 10.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [00:23<00:04, 10.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [00:24<00:05,  9.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [00:24<00:04,  9.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [00:24<00:04, 10.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [00:24<00:03, 10.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [00:24<00:03,  9.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [00:25<00:03,  9.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [00:25<00:03,  9.82it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [00:25<00:03, 10.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [00:25<00:03,  9.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [00:25<00:02, 10.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [00:26<00:02, 10.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [00:26<00:02,  9.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [00:26<00:02, 10.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:26<00:01, 10.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:26<00:01, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:26<00:01, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:27<00:01, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:27<00:01, 10.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:27<00:01, 10.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:27<00:00, 10.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:28<00:00,  9.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:28<00:00,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:28<00:00, 10.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:28<00:00, 10.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:28<00:00, 10.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is hateful. Please respond with only False (not hateful) or True (very hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_hate'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_hate'] = df_normalized['Llama_hate'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama_combined\n",
      "False    186\n",
      "True     114\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.3643410852713178\n",
      "Llama_toxic\n",
      "False    271\n",
      "True      29\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.09929078014184398\n",
      "Llama_hate\n",
      "False    251\n",
      "True      49\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.21138211382113822\n"
     ]
    }
   ],
   "source": [
    "print(df_normalized['Llama_combined'].value_counts())\n",
    "# print(df_normalized['combined'].value_counts())\n",
    "f1 = f1_score(df_normalized['combined'], df_normalized['Llama_combined'])\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_toxic'].value_counts())\n",
    "f1 = f1_score(df_normalized['toxic'], df_normalized['Llama_toxic'])\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_hate'].value_counts())\n",
    "f1 = f1_score(df_normalized['hateful'], df_normalized['Llama_hate'])\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model='meta-llama/Llama-3.2-3B-Instruct', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:03<16:03,  3.22s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:05<14:21,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:08<13:41,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:11<13:31,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:14<13:37,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:16<13:19,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:19<13:01,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:21<12:57,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:24<12:55,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:27<13:06,  2.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:30<13:06,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:32<13:05,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:35<13:03,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:38<13:17,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:41<13:22,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:44<13:33,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:47<13:31,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:50<13:31,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:53<13:46,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:56<13:38,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:58<13:27,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [01:01<13:21,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [01:04<13:13,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [01:07<13:02,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [01:10<12:59,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [01:12<12:45,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [01:15<12:42,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [01:18<12:33,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [01:21<12:20,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [01:23<12:11,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [01:26<12:14,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [01:29<12:08,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [01:31<12:08,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [01:34<12:08,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [01:37<12:33,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [01:40<12:24,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [01:43<12:15,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [01:46<12:49,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [01:49<12:39,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [01:51<12:08,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [01:54<11:53,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [01:57<11:34,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [01:59<11:31,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [02:02<11:34,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [02:05<11:24,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [02:07<11:23,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [02:10<11:23,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [02:13<11:16,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [02:16<11:19,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [02:18<11:25,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [02:21<11:32,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [02:24<11:21,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [02:27<11:10,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [02:29<11:07,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [02:32<11:11,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [02:35<11:02,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [02:37<10:59,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [02:40<10:49,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [02:43<10:54,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [02:45<10:44,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [02:48<10:49,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [02:51<10:39,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [02:54<10:35,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [02:56<10:41,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [02:59<10:30,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [03:02<10:20,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [03:04<10:23,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [03:07<10:16,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [03:09<10:07,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [03:12<10:10,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [03:15<10:04,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [03:17<10:01,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [03:20<10:04,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [03:23<10:01,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [03:25<09:58,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [03:28<10:05,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [03:31<10:02,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [03:34<09:55,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [03:36<09:58,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [03:39<09:55,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [03:42<09:53,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [03:44<09:50,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [03:47<09:47,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [03:50<09:44,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [03:53<09:41,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [03:55<09:41,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [03:58<09:39,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [04:01<09:44,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [04:04<09:45,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [04:06<09:42,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [04:09<09:44,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [04:12<09:41,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [04:15<09:42,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [04:18<09:34,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [04:21<09:42,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [04:23<09:34,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [04:26<09:20,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [04:29<09:17,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [04:31<09:09,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [04:34<08:57,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [04:37<08:57,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [04:40<09:01,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [04:42<08:51,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [04:45<08:55,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [04:48<09:01,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [04:51<09:02,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [04:53<08:48,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [04:56<08:45,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [04:59<08:35,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [05:01<08:31,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [05:04<08:27,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [05:07<08:20,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [05:09<08:19,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [05:12<08:17,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [05:15<08:12,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [05:17<08:13,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [05:20<08:04,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [05:22<07:53,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [05:25<07:52,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [05:28<07:56,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [05:30<07:45,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [05:33<07:59,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [05:36<07:45,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [05:38<07:36,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [05:41<07:47,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 126/300 [05:44<07:34,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [05:46<07:25,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 128/300 [05:49<07:23,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [05:51<07:12,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 130/300 [05:54<07:10,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [05:56<07:10,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 132/300 [05:59<07:04,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [06:01<07:02,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▍     | 134/300 [06:04<07:08,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [06:06<07:00,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 136/300 [06:09<06:59,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [06:11<06:56,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 138/300 [06:14<06:48,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [06:16<06:44,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 140/300 [06:19<06:42,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [06:21<06:38,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 142/300 [06:24<06:36,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [06:26<06:31,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 144/300 [06:29<06:28,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [06:31<06:26,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▊     | 146/300 [06:34<06:24,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [06:36<06:20,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 148/300 [06:39<06:19,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [06:41<06:21,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 150/300 [06:44<06:14,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [06:46<06:11,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 152/300 [06:49<06:11,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [06:51<06:06,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████▏    | 154/300 [06:54<06:05,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [06:56<06:03,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 156/300 [06:59<06:01,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [07:01<05:57,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 158/300 [07:04<05:54,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [07:06<05:53,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 160/300 [07:09<05:52,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [07:11<05:49,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 162/300 [07:14<05:45,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [07:16<05:45,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▍    | 164/300 [07:19<05:41,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [07:22<05:41,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 166/300 [07:24<05:36,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [07:27<05:36,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 168/300 [07:29<05:37,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [07:32<05:33,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 170/300 [07:34<05:31,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [07:37<05:27,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 172/300 [07:39<05:23,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [07:42<05:23,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 174/300 [07:44<05:17,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [07:47<05:13,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▊    | 176/300 [07:49<05:14,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [07:52<05:12,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 178/300 [07:54<05:07,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [07:57<05:06,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 180/300 [07:59<05:01,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [08:02<05:00,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 182/300 [08:05<04:56,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [08:07<04:53,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████▏   | 184/300 [08:10<04:53,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [08:12<04:48,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 186/300 [08:15<04:47,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [08:17<04:46,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 188/300 [08:20<04:40,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [08:22<04:38,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 190/300 [08:25<04:35,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [08:27<04:32,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 192/300 [08:30<04:33,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [08:32<04:27,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▍   | 194/300 [08:35<04:24,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [08:37<04:22,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 196/300 [08:40<04:18,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [08:42<04:16,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 198/300 [08:45<04:14,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [08:47<04:13,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 200/300 [08:50<04:11,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [08:52<04:07,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 202/300 [08:55<04:07,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [08:58<04:12,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 204/300 [09:00<04:07,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [09:03<04:02,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▊   | 206/300 [09:05<03:58,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [09:08<03:57,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 208/300 [09:10<03:54,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [09:13<03:51,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 210/300 [09:15<03:46,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [09:18<03:46,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 212/300 [09:20<03:43,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [09:23<03:40,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████▏  | 214/300 [09:25<03:34,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [09:28<03:30,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 216/300 [09:30<03:25,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [09:32<03:22,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 218/300 [09:35<03:18,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [09:37<03:15,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 220/300 [09:40<03:12,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [09:42<03:11,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 222/300 [09:45<03:11,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [09:47<03:08,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▍  | 224/300 [09:49<03:04,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [09:52<03:02,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 226/300 [09:54<02:58,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [09:57<02:55,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 228/300 [09:59<02:53,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [10:02<02:52,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 230/300 [10:04<02:50,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [10:06<02:46,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 232/300 [10:09<02:43,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [10:11<02:41,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 234/300 [10:14<02:39,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [11:14<21:21, 19.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▊  | 236/300 [11:16<15:32, 14.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [11:19<11:30, 10.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 238/300 [11:21<08:42,  8.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [11:24<06:46,  6.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 240/300 [11:26<05:23,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [11:29<04:29,  4.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 242/300 [11:32<03:54,  4.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [11:34<03:23,  3.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████▏ | 244/300 [11:37<03:02,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [11:39<02:47,  3.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 246/300 [11:42<02:35,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [11:44<02:26,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 248/300 [11:47<02:21,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [11:50<02:22,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 250/300 [11:53<02:19,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [11:55<02:11,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 252/300 [11:58<02:07,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [12:00<02:01,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▍ | 254/300 [12:03<01:58,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [12:05<01:54,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 256/300 [12:08<01:51,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [12:10<01:49,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 258/300 [12:13<01:45,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [12:15<01:44,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 260/300 [12:18<01:40,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [12:20<01:39,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 262/300 [12:23<01:37,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [12:25<01:34,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 264/300 [12:28<01:33,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [12:31<01:30,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▊ | 266/300 [12:33<01:25,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [12:36<01:23,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 268/300 [12:38<01:21,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [12:41<01:18,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 270/300 [12:43<01:17,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [12:46<01:13,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 272/300 [12:48<01:11,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [12:51<01:08,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████▏| 274/300 [12:53<01:05,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [12:56<01:04,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 276/300 [12:59<01:01,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [13:01<00:58,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 278/300 [13:04<00:56,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [13:06<00:53,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 280/300 [13:09<00:50,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [13:11<00:48,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 282/300 [13:14<00:45,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [13:16<00:42,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▍| 284/300 [13:19<00:40,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [13:21<00:37,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 286/300 [13:24<00:34,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [13:26<00:32,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 288/300 [13:29<00:30,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [13:31<00:27,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 290/300 [13:34<00:24,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [13:36<00:22,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 292/300 [13:39<00:19,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [13:41<00:17,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 294/300 [13:44<00:16,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [13:47<00:13,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▊| 296/300 [13:49<00:10,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [13:52<00:07,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 298/300 [13:54<00:05,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [13:57<00:02,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [14:00<00:00,  2.80s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic or hateful. Please respond with only False (not toxic or hateful) or True (very toxic or hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_combined'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_combined'] = df_normalized['Llama_combined'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:02<12:55,  2.59s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:05<12:45,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:07<12:37,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:10<12:20,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:12<12:49,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:15<12:41,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:17<12:31,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:20<12:20,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:22<12:11,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:25<12:04,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:27<12:10,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:30<12:05,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:32<11:57,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:35<11:50,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:37<11:57,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:40<12:02,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:43<11:57,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:45<11:43,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:48<12:06,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:50<11:56,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:53<11:42,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:55<11:39,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:58<11:25,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [01:00<11:20,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [01:03<11:24,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [01:05<11:12,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [01:07<11:10,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [01:10<11:07,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [01:12<11:02,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [01:15<11:02,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [01:17<11:01,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [01:20<10:57,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [01:22<10:53,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [01:25<11:05,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [01:27<11:01,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [01:30<10:53,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [01:32<10:50,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [01:35<11:05,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [01:37<10:53,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [01:40<10:42,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [01:42<10:37,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [01:44<10:31,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [01:47<10:27,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [01:49<10:21,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [01:52<10:25,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [01:54<10:14,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [01:57<10:10,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [01:59<10:12,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [02:01<10:10,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [02:04<10:15,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [02:07<10:18,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [02:09<10:18,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [02:12<10:15,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [02:14<10:10,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [02:16<10:09,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [02:19<10:24,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [02:22<10:21,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [02:24<10:14,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [02:27<10:00,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [02:29<10:03,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [02:32<09:53,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [02:34<09:45,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [02:37<09:46,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [02:39<09:43,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [02:41<09:39,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [02:44<09:34,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [02:46<09:35,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [02:49<09:30,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [02:51<09:29,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [02:54<09:28,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [02:56<09:21,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [02:59<09:15,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [03:01<09:15,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [03:04<09:14,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [03:06<09:15,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [03:08<09:08,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [03:11<09:06,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [03:13<09:07,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [03:16<08:59,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [03:18<08:57,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [03:21<08:52,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [03:23<08:46,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [03:25<08:46,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [03:28<08:50,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [03:31<09:01,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [03:33<08:55,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [03:36<08:48,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [03:38<08:50,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [03:41<08:46,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [03:43<08:45,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [03:46<08:49,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [03:48<08:38,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [03:50<08:27,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [03:53<08:28,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [03:55<08:28,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [03:58<08:26,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [04:00<08:18,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [04:03<08:19,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [04:05<08:16,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [04:08<08:12,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [04:10<08:15,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [04:13<08:15,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [04:15<08:15,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [04:18<08:10,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [04:20<08:05,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [04:23<08:05,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [04:25<08:04,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [04:28<07:53,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [04:30<07:51,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [04:33<07:44,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [04:35<07:37,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [04:37<07:34,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [04:40<07:36,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [04:42<07:36,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [04:45<07:36,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [04:47<07:33,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [04:50<07:27,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [04:52<07:22,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [04:55<07:22,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [04:57<07:25,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [05:00<07:19,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [05:02<07:37,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [05:05<07:28,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [05:07<07:20,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [05:10<07:34,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 126/300 [05:13<07:27,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [05:15<07:18,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 128/300 [05:18<07:14,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [05:20<07:09,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 130/300 [05:22<07:04,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [05:25<07:00,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 132/300 [05:27<06:54,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [05:30<06:53,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▍     | 134/300 [05:32<06:51,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [05:35<06:47,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 136/300 [05:37<06:51,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [05:40<06:48,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 138/300 [05:42<06:42,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [05:45<06:44,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 140/300 [05:47<06:41,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [05:50<06:39,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 142/300 [05:52<06:34,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [05:55<06:28,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 144/300 [05:57<06:31,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [06:00<06:23,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▊     | 146/300 [06:02<06:19,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [06:05<06:18,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 148/300 [06:07<06:14,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [06:10<06:14,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 150/300 [06:12<06:09,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [06:15<06:06,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 152/300 [06:17<06:07,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [06:20<06:01,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████▏    | 154/300 [06:22<05:57,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [06:24<05:55,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 156/300 [06:27<05:51,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [06:29<05:47,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 158/300 [06:32<05:42,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [06:34<05:46,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 160/300 [06:37<05:42,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [06:39<05:39,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 162/300 [06:41<05:39,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [06:44<05:36,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▍    | 164/300 [06:46<05:36,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [06:49<05:35,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 166/300 [06:51<05:31,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [06:54<05:31,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 168/300 [06:56<05:27,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [06:59<05:24,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 170/300 [07:01<05:24,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [07:04<05:17,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 172/300 [07:06<05:11,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [07:09<05:11,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 174/300 [07:11<05:08,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [07:14<05:10,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▊    | 176/300 [07:16<05:07,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [07:19<05:04,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 178/300 [07:21<05:03,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [07:24<05:00,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 180/300 [07:26<05:03,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [07:29<04:58,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 182/300 [07:31<04:55,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [07:34<04:48,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████▏   | 184/300 [07:36<04:40,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [07:38<04:36,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 186/300 [07:41<04:37,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [07:43<04:33,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 188/300 [07:46<04:30,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [07:48<04:27,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 190/300 [07:50<04:26,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [07:53<04:22,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 192/300 [07:55<04:17,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [07:57<04:15,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▍   | 194/300 [08:00<04:12,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [08:02<04:10,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 196/300 [08:05<04:11,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [08:07<04:10,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 198/300 [08:10<04:10,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [08:12<04:07,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 200/300 [08:15<04:06,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [08:17<04:03,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 202/300 [08:20<04:04,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [08:22<04:10,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 204/300 [08:25<04:07,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [08:27<04:01,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▊   | 206/300 [08:30<03:56,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [08:32<03:55,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 208/300 [08:35<03:49,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [08:37<03:45,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 210/300 [08:40<03:42,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [08:42<03:40,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 212/300 [08:45<03:39,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [08:47<03:37,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████▏  | 214/300 [08:50<03:35,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [08:52<03:32,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 216/300 [08:55<03:29,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [08:57<03:27,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 218/300 [09:00<03:25,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [09:02<03:22,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 220/300 [09:05<03:19,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [09:07<03:15,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 222/300 [09:10<03:11,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [09:12<03:08,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▍  | 224/300 [09:15<03:06,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [09:17<03:04,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 226/300 [09:19<03:01,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [09:22<03:00,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 228/300 [09:24<02:57,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [09:27<02:55,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 230/300 [09:29<02:52,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [09:32<02:49,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 232/300 [09:34<02:46,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [09:37<02:43,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 234/300 [09:39<02:42,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [10:39<21:12, 19.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▊  | 236/300 [10:41<15:23, 14.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [10:44<11:21, 10.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 238/300 [10:46<08:33,  8.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [10:48<06:38,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 240/300 [10:51<05:17,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [10:53<04:25,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 242/300 [10:56<03:50,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [10:59<03:21,  3.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████▏ | 244/300 [11:01<02:58,  3.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [11:03<02:42,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 246/300 [11:06<02:32,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [11:08<02:23,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 248/300 [11:11<02:16,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [11:13<02:11,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 250/300 [11:16<02:07,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [11:18<02:04,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 252/300 [11:21<02:04,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [11:23<02:00,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▍ | 254/300 [11:26<01:56,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [11:28<01:53,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 256/300 [11:31<01:50,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [11:33<01:47,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 258/300 [11:36<01:43,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [11:38<01:41,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 260/300 [11:41<01:38,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [11:43<01:36,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 262/300 [11:46<01:34,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [11:48<01:31,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 264/300 [11:51<01:29,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [11:53<01:27,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▊ | 266/300 [11:56<01:25,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [11:58<01:22,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 268/300 [12:01<01:20,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [12:03<01:18,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 270/300 [12:06<01:16,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [12:08<01:13,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 272/300 [12:11<01:10,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [12:13<01:07,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████▏| 274/300 [12:16<01:04,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [12:18<01:03,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 276/300 [12:21<01:00,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [12:23<00:56,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 278/300 [12:26<00:54,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [12:28<00:52,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 280/300 [12:31<00:49,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [12:33<00:46,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 282/300 [12:36<00:44,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [12:38<00:41,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▍| 284/300 [12:40<00:39,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [12:43<00:36,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 286/300 [12:45<00:34,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [12:48<00:32,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 288/300 [12:50<00:29,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [12:53<00:27,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 290/300 [12:55<00:24,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [12:58<00:22,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 292/300 [13:01<00:21,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [13:03<00:18,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 294/300 [13:06<00:15,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [13:08<00:12,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▊| 296/300 [13:11<00:10,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [13:13<00:07,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 298/300 [13:16<00:04,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [13:18<00:02,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [13:20<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic. Please respond with only False (not toxic) or True (very toxic) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_toxic'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_toxic'] = df_normalized['Llama_toxic'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:02<12:19,  2.47s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:04<12:10,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:07<12:12,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:09<12:05,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:12<12:18,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:14<12:11,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:17<12:05,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:19<12:05,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:22<12:03,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:24<11:56,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:27<12:05,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:29<11:55,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:32<11:46,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:34<11:48,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:37<11:48,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:39<11:54,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:42<11:45,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:44<11:36,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:47<12:03,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:49<11:51,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:52<11:47,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:55<11:47,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:57<11:34,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:59<11:28,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [01:02<11:32,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [01:04<11:23,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [01:07<11:18,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [01:09<11:15,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [01:12<11:19,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [01:14<11:11,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [01:17<11:08,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [01:19<11:14,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [01:22<11:10,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [01:24<11:11,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [01:27<11:02,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [01:29<10:55,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [01:32<10:50,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [01:35<11:09,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [01:37<10:58,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [01:40<10:55,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [01:42<10:55,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [01:45<10:47,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [01:47<10:42,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [01:49<10:35,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [01:52<10:31,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [01:54<10:25,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [01:57<10:27,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [01:59<10:34,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [02:02<10:32,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [02:05<10:30,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [02:07<10:33,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [02:10<10:26,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [02:12<10:23,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [02:15<10:15,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [02:17<10:06,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [02:20<10:14,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [02:22<10:09,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [02:25<10:01,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [02:27<10:04,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [02:30<09:58,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [02:32<10:00,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [02:35<09:57,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [02:37<09:58,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [02:40<09:46,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [02:42<09:35,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [02:44<09:25,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [02:47<09:23,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [02:49<09:22,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [02:52<09:26,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [02:54<09:24,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [02:57<09:26,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [02:59<09:26,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [03:02<09:25,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [03:04<09:23,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [03:07<09:22,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [03:09<09:19,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [03:12<09:17,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [03:14<09:10,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [03:17<09:08,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [03:19<09:09,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [03:22<09:04,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [03:24<08:58,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [03:27<09:02,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [03:29<08:56,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [03:32<08:57,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [03:34<08:55,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [03:37<08:52,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [03:39<08:55,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [03:42<08:45,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [03:44<08:40,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [03:47<08:42,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [03:49<08:35,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [03:52<08:39,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [03:54<08:33,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [03:57<08:35,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [03:59<08:29,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [04:01<08:24,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [04:04<08:19,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [04:07<08:25,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [04:09<08:20,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [04:12<08:21,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [04:14<08:24,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [04:17<08:17,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [04:19<08:15,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [04:22<08:08,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [04:24<08:06,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [04:27<08:05,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [04:29<07:57,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [04:32<07:59,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [04:34<07:52,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [04:37<07:49,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [04:39<07:50,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [04:42<07:45,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [04:44<07:43,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [04:47<07:43,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [04:49<07:39,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [04:52<07:39,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [04:54<07:32,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [04:57<07:31,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [04:59<07:36,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [05:02<07:31,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [05:05<07:48,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [05:07<07:38,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [05:10<07:29,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [05:12<07:40,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 126/300 [05:15<07:30,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [05:17<07:23,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 128/300 [05:20<07:18,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [05:22<07:13,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 130/300 [05:25<07:08,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [05:27<07:08,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 132/300 [05:30<06:58,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [05:32<06:54,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▍     | 134/300 [05:35<07:05,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [05:38<06:57,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 136/300 [05:40<06:56,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [05:43<06:57,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 138/300 [05:45<06:50,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [05:48<06:47,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 140/300 [05:50<06:41,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [05:53<06:38,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 142/300 [05:55<06:36,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [05:58<06:31,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 144/300 [06:00<06:31,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [06:03<06:32,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▊     | 146/300 [06:05<06:28,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [06:08<06:27,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 148/300 [06:10<06:24,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [06:13<06:22,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 150/300 [06:15<06:18,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [06:18<06:13,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 152/300 [06:20<06:09,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [06:23<06:06,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████▏    | 154/300 [06:25<06:01,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [06:28<06:02,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 156/300 [06:30<06:02,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [06:33<06:02,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 158/300 [06:36<06:02,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [06:38<05:57,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 160/300 [06:41<05:55,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [06:43<05:54,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 162/300 [06:46<05:47,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [06:48<05:43,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▍    | 164/300 [06:51<05:45,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [06:53<05:41,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 166/300 [06:56<05:36,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [06:58<05:38,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 168/300 [07:01<05:37,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [07:03<05:33,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 170/300 [07:06<05:34,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [07:08<05:28,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 172/300 [07:11<05:25,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [07:14<05:24,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 174/300 [07:16<05:18,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [07:19<05:19,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▊    | 176/300 [07:21<05:15,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [07:24<05:11,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 178/300 [07:26<05:08,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [07:29<05:03,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 180/300 [07:31<05:02,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [07:34<05:01,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 182/300 [07:36<04:55,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [07:39<04:55,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████▏   | 184/300 [07:41<04:49,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [07:44<04:45,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 186/300 [07:46<04:44,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [07:49<04:40,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 188/300 [07:51<04:36,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [07:54<04:35,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 190/300 [07:56<04:32,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [07:59<04:31,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 192/300 [08:01<04:28,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [08:04<04:26,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▍   | 194/300 [08:06<04:25,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [08:09<04:22,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 196/300 [08:11<04:21,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [08:14<04:18,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 198/300 [08:16<04:15,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [08:19<04:14,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 200/300 [08:21<04:11,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [08:24<04:09,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 202/300 [08:26<04:10,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [08:29<04:12,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 204/300 [08:32<04:09,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [08:34<04:05,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▊   | 206/300 [08:37<04:00,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [08:39<04:03,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 208/300 [08:42<03:59,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [08:45<03:54,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 210/300 [08:47<03:52,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [08:50<03:50,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 212/300 [08:52<03:46,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [08:55<03:43,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████▏  | 214/300 [08:57<03:38,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [09:00<03:34,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 216/300 [09:02<03:31,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [09:05<03:28,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 218/300 [09:07<03:27,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [09:10<03:24,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 220/300 [09:12<03:21,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [09:15<03:18,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 222/300 [09:17<03:15,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [09:20<03:11,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▍  | 224/300 [09:22<03:10,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [09:25<03:07,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 226/300 [09:27<03:06,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [09:30<03:02,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 228/300 [09:32<03:00,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [09:35<03:01,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 230/300 [09:38<02:58,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [09:40<02:54,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 232/300 [09:43<02:51,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [09:45<02:46,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 234/300 [09:48<02:45,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [10:47<21:20, 19.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▊  | 236/300 [10:50<15:30, 14.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [10:52<11:28, 10.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 238/300 [10:55<08:42,  8.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [10:57<06:45,  6.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 240/300 [11:00<05:24,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [11:03<04:30,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 242/300 [11:05<03:53,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [11:08<03:23,  3.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████▏ | 244/300 [11:10<03:01,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [11:13<02:46,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 246/300 [11:15<02:36,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [11:18<02:26,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 248/300 [11:21<02:20,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [11:23<02:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 250/300 [11:26<02:10,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [11:28<02:05,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 252/300 [11:31<02:05,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [11:33<02:00,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▍ | 254/300 [11:36<01:58,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [11:38<01:55,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 256/300 [11:41<01:51,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [11:43<01:49,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 258/300 [11:46<01:45,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [11:48<01:43,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 260/300 [11:51<01:42,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [11:54<01:39,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 262/300 [11:56<01:36,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [11:59<01:33,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 264/300 [12:01<01:31,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [12:04<01:29,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▊ | 266/300 [12:06<01:26,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [12:09<01:24,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 268/300 [12:11<01:22,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [12:14<01:19,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 270/300 [12:16<01:16,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [12:19<01:13,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 272/300 [12:21<01:10,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [12:24<01:08,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████▏| 274/300 [12:27<01:05,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [12:29<01:04,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 276/300 [12:32<01:01,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [12:34<00:58,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 278/300 [12:37<00:55,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [12:39<00:53,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 280/300 [12:42<00:50,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [12:44<00:48,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 282/300 [12:47<00:45,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [12:49<00:42,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▍| 284/300 [12:52<00:40,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [12:54<00:37,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 286/300 [12:57<00:35,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [13:00<00:33,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 288/300 [13:02<00:30,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [13:05<00:27,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 290/300 [13:07<00:25,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [13:10<00:22,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 292/300 [13:12<00:20,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [13:15<00:17,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 294/300 [13:17<00:15,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [13:20<00:12,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▊| 296/300 [13:22<00:10,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [13:25<00:07,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 298/300 [13:27<00:05,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [13:30<00:02,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [13:32<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is hateful. Please respond with only False (not hateful) or True (very hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_hate'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_hate'] = df_normalized['Llama_hate'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama_combined\n",
      "False    212\n",
      "True      88\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.5344827586206896\n",
      "Llama_toxic\n",
      "False    197\n",
      "True     103\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.4930232558139535\n",
      "Llama_hate\n",
      "False    251\n",
      "True      49\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.3902439024390244\n"
     ]
    }
   ],
   "source": [
    "print(df_normalized['Llama_combined'].value_counts())\n",
    "# print(df_normalized['combined'].value_counts())\n",
    "f1 = f1_score(df_normalized['combined'], df_normalized['Llama_combined'])\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_toxic'].value_counts())\n",
    "f1 = f1_score(df_normalized['toxic'], df_normalized['Llama_toxic'])\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_hate'].value_counts())\n",
    "f1 = f1_score(df_normalized['hateful'], df_normalized['Llama_hate'])\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.73s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model='aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:24<2:00:27, 24.17s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [01:58<52:18, 10.82s/it] You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [53:04<00:00, 10.61s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic or hateful. Please respond with only False (not toxic or hateful) or True (very toxic or hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_combined'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_combined'] = df_normalized['Llama_combined'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:10<53:43, 10.78s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [53:01<00:00, 10.61s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic. Please respond with only False (not toxic) or True (very toxic) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_toxic'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_toxic'] = df_normalized['Llama_toxic'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:09<49:24,  9.92s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [50:46<00:00, 10.15s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is hateful. Please respond with only False (not hateful) or True (very hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_hate'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_hate'] = df_normalized['Llama_hate'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama_combined\n",
      "False    220\n",
      "True      80\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.5178571428571429\n",
      "Llama_toxic\n",
      "False    202\n",
      "True      98\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.5142857142857142\n",
      "Llama_hate\n",
      "False    257\n",
      "True      43\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.3247863247863248\n"
     ]
    }
   ],
   "source": [
    "print(df_normalized['Llama_combined'].value_counts())\n",
    "# print(df_normalized['combined'].value_counts())\n",
    "f1 = f1_score(df_normalized['combined'], df_normalized['Llama_combined'])\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_toxic'].value_counts())\n",
    "f1 = f1_score(df_normalized['toxic'], df_normalized['Llama_toxic'])\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_hate'].value_counts())\n",
    "f1 = f1_score(df_normalized['hateful'], df_normalized['Llama_hate'])\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
