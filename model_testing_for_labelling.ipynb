{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please create a folder named `data` in the main directory and place the following files inside it:\n",
    "\n",
    "- `Reddit-Threads_2020-2021.csv`\n",
    "- `Reddit-Threads_2022-2023.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\caboo\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Device in use: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# NLP and Transformers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# API and Hugging Face Integration\n",
    "import requests\n",
    "from huggingface_hub import login\n",
    "\n",
    "# AI APIs\n",
    "import google.generativeai as genai\n",
    "from googleapiclient import discovery\n",
    "from openai import OpenAI\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# huggingface API key\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "login(token=hf_api_key)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f'Device in use: {device_name}')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Device in use: CPU')\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        timestamp  \\\n",
      "0     Expensive eh now that Uglyfoods closed down :(   30/1/2023 1:04   \n",
      "1                How dare you.. wan go lim kopi ah??   4/5/2022 18:57   \n",
      "2  Yeah the governments can politick all they wan...  28/6/2022 13:44   \n",
      "3               Hijacks event, then complains. Wild.   12/7/2022 7:29   \n",
      "4  Hate to break it to you. But once someone accu...   23/8/2023 2:08   \n",
      "\n",
      "              username                                               link  \\\n",
      "0      MangoDangoLango  /r/singapore/comments/10nqt5h/rsingapore_rando...   \n",
      "1               900122  /r/SingaporeRaw/comments/ui0rmg/dont_take_offe...   \n",
      "2  DisillusionedSinkie  /r/singapore/comments/vmb197/malaysias_top_tal...   \n",
      "3            nehjipain  /r/singapore/comments/vx42x1/nus_student_tried...   \n",
      "4          KeenStudent  /r/singapore/comments/15ybdme/sorry_doesnt_cut...   \n",
      "\n",
      "      link_id   parent_id       id subreddit_id  \\\n",
      "0  t3_10nqt5h  t1_j6dwxo8  j6fuv4x     t5_2qh8c   \n",
      "1   t3_ui0rmg  t1_i79scst  i7bsqea     t5_xnx04   \n",
      "2   t3_vmb197  t1_ie1fiyf  ie1ycm0     t5_2qh8c   \n",
      "3   t3_vx42x1  t1_iftsm0q  iftwbcz     t5_2qh8c   \n",
      "4  t3_15ybdme  t1_jxc6g7p  jxcxjd6     t5_2qh8c   \n",
      "\n",
      "                                          moderation  toxic  hateful  combined  \n",
      "0  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "1  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "2  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "3  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "4  {'banned_at_utc': None, 'mod_reason_by': None,...  False    False     False  \n",
      "(300, 12)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "###   SMALL DATASET   ###\n",
    "# df = pd.read_csv('data/Reddit-Threads_2020-2021.csv', nrows=10000)\n",
    "# print(df.head())\n",
    "###   SMALL DATASET   ###\n",
    "\n",
    "###   FULL DATASET   ###\n",
    "# chunk_size = 10000\n",
    "# for chunk in pd.read_csv('data/Reddit-Threads_2020-2021.csv', chunksize=chunk_size):\n",
    "#     print(chunk.head())  \n",
    "#     df = pd.concat([df, chunk])\n",
    "# for chunk in pd.read_csv('data/Reddit-Threads_2022-2023.csv', chunksize=chunk_size):\n",
    "#     print(chunk.head())  \n",
    "#     df = pd.concat([df, chunk])\n",
    "###   FULL DATASET   ###\n",
    "\n",
    "###   VALIDATION DATASET   ###\n",
    "df = pd.read_csv('data/labeled_data_2.csv')\n",
    "df['combined'] = df['hateful'] | df['toxic']\n",
    "print(df.head())\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['moderation'] = df['moderation'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "# moderation_dicts = df['moderation']\n",
    "# moderation_normalized = pd.json_normalize(moderation_dicts)\n",
    "# # print(moderation_normalized)\n",
    "# df = df.reset_index(drop=True)\n",
    "# moderation_normalized = moderation_normalized.reset_index(drop=True)\n",
    "# df_normalized = pd.concat([df.drop(columns=['moderation']), moderation_normalized], axis=1)\n",
    "# # print(df_normalized.columns)\n",
    "df_normalized = df\n",
    "\n",
    "### removing deleted or removed text ###\n",
    "df_normalized = df_normalized[df_normalized['text'] != '[deleted]']\n",
    "df_normalized = df_normalized[df_normalized['text'] != '[removed]']\n",
    "df_normalized = df_normalized.dropna(subset=['text'])\n",
    "### removing deleted or removed text ###\n",
    "\n",
    "### stop word removal ###\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stop_words(text):\n",
    "#     if isinstance(text, str):  # Check if the text is a string\n",
    "#         return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "#     return text \n",
    "\n",
    "# df_normalized['text'] = df_normalized['text'].apply(remove_stop_words)\n",
    "# print(df_normalized['text'])\n",
    "# print(stop_words)\n",
    "### stop word removal ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Finding the best labeller\n",
    "https://huggingface.co/sileod/deberta-v3-base-tasksource-toxicity\n",
    "\n",
    "after testing with the following models:<br>\n",
    "with the threshold at 0 meaning 100% True or if not 1 False and 299 True<br>\n",
    "As sileod/deberta-v3-base-tasksource-toxicity has a relatively high f1 score and a takes a relatively low time to label the text data, we decided to use it. <br>\n",
    "| models                                                                          |   best toxic f1_score |   toxic threshold |   best hate f1_score |   hate threshold |   combined best f1 score |   combined threshold  | time taken   |\n",
    "|----------------------------------------------------------------------------------|----------------------:|------------------:|---------------------:|-----------------:|-------------------------:|----------------------:|:-------------|\n",
    "| sileod/deberta-v3-base-tasksource-toxicity                                       |             0.547368  |              0.01 |             0.573034 |             0.04 |                 0.675079 |                  0.01 | 11s          |\n",
    "| unitary/toxic-bert                                                               |             0.543689  |              0    |             0.513889 |             0.40 |                 0.648649 |                  0    | 4s           |\n",
    "| GroNLP/hateBERT                                                                  |             0.545012  |              0.38 |             0.411552 |             0.72 |                 0.648649 |                  0    | 4s           |\n",
    "| textdetox/xlmr-large-toxicity-classifier                                         |             0.540146  |              0    |             0.493671 |             0.05 |                 0.645598 |                  0    | 4s           |\n",
    "| facebook/roberta-hate-speech-dynabench-r4-target                                 |             0.540146  |              0    |             0.42236  |             0.04 |                 0.645598 |                  0    | 4s           |\n",
    "| cointegrated/rubert-tiny-toxicity                                                |             0.540146  |              0    |             0.429268 |             0.04 |                 0.645598 |                  0    | 1s           |\n",
    "| badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification  |             0.540146  |              0    |             0.391421 |             0    |                 0.645598 |                  0    | 2s           |\n",
    "| citizenlab/distilbert-base-multilingual-cased-toxicity                           |             0.540146  |              0    |             0.48062  |             0.57 |                 0.645598 |                  0    | 7s           |\n",
    "| meta-llama/Llama-3.2-1B-Instruct                                                 |             0.0610687 |            nan    |             0.14     |           nan    |                 0.27027  |                nan    | 27s          |\n",
    "| meta-llama/Llama-3.2-3B-Instruct                                                 |             0.46875   |            nan    |             0.342342 |           nan    |                 0.497778 |                nan    | 12min 7s     |\n",
    "| aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct                                  |             0.514286  |            nan    |             0.324786 |           nan    |                 0.517857 |                nan    | 1h 40mins    |\n",
    "| Perspective API                                                                  |             0.289157  |            nan    |             0.289157 |           nan    |                 0.424242 |                nan    | 6mins 20s    |\n",
    "| GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1                      |             0.50411   |            nan    |             0.385321 |           nan    |                 0.604534 |                nan    | 15s          |\n",
    "| Hate-speech-CNERG/dehatebert-mono-english                                        |             0.543689  |              0    |             0.47619  |             0.08 |                 0.648649 |                  0    | 5s           |\n",
    "| cardiffnlp/twitter-roberta-base-hate                                             |             0.540146  |              0    |             0.423077 |             0.04 |                 0.645598 |                  0    | 5s           |\n",
    "| Hate-speech-CNERG/bert-base-uncased-hatexplain                                   |             0.574468  |              0.73 |             0.476744 |             0.67 |                 0.660661 |                  0.74 | 7s           |\n",
    "| mrm8488/distilroberta-finetuned-tweets-hate-speech                               |             0.556122  |              0.04 |             0.391421 |             0    |                 0.646226 |                  0.04 | 3s           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Trying Bert models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 sileod/deberta-v3-base-tasksource-toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:12<00:00, 24.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.01, Best F1 Score: 0.5473684210526316\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.5730337078651685\n",
      "Best Threshold for combined: 0.01, Best F1 Score: 0.6750788643533123\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     173\n",
      "False    127\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    196\n",
      "True     104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     173\n",
      "False    127\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'sileod/deberta-v3-base-tasksource-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:11<00:00, 25.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'sileod/deberta-v3-base-tasksource-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'temp_score' less than 0.2: 26\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows where 'temp_score' is less than 0.2\n",
    "count_below_0_2 = df_normalized[df_normalized['temp_score'] >= 0.8].shape[0]\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of rows with 'temp_score' less than 0.2: {count_below_0_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   temp_score toxicity_label\n",
      "0    0.008786      not toxic\n",
      "1    0.005781      not toxic\n",
      "2    0.003931      not toxic\n",
      "3    0.008570      not toxic\n",
      "4    0.094989      not toxic\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'toxicity_label' based on the conditions for 'temp_score'\n",
    "df_normalized['toxicity_label'] = df_normalized['temp_score'].apply(\n",
    "    lambda x: 'toxic' if x >= 0.1 else ('not toxic' if x < 0.2 else 'ambiguous')\n",
    ")\n",
    "\n",
    "# Print the first few rows to verify the new column\n",
    "print(df_normalized[['temp_score', 'toxicity_label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   combined combined_label\n",
      "0     False      not toxic\n",
      "1     False      not toxic\n",
      "2     False      not toxic\n",
      "3     False      not toxic\n",
      "4     False      not toxic\n"
     ]
    }
   ],
   "source": [
    "# Relabel the 'combined' column based on the condition\n",
    "df_normalized['combined_label'] = df_normalized['combined'].apply(lambda x: 'toxic' if x else 'not toxic')\n",
    "\n",
    "# Print the first few rows to verify the new column\n",
    "print(df_normalized[['combined', 'combined_label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6207944220919543\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_normalized['combined_label'], df_normalized['toxicity_label'], average='weighted'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(recall[\u001b[38;5;241m0\u001b[39m], precision[\u001b[38;5;241m0\u001b[39m], marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot Toxic/Ambiguous vs Toxic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mrecall\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, precision[\u001b[38;5;241m1\u001b[39m], marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmbiguous vs Toxic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Step 5: Customize and show the plot\u001b[39;00m\n\u001b[0;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Step 1: Map 'combined_label' and 'toxicity_label' into numeric values\n",
    "# toxic = 2, not toxic = 1, ambiguous = 1\n",
    "label_mapping = {'toxic': 2, 'not toxic': 0, 'ambiguous': 1}\n",
    "y_true = df_normalized['combined_label'].map(label_mapping)\n",
    "y_pred = df_normalized['toxicity_label'].map(label_mapping)\n",
    "\n",
    "# Step 2: Binarize the output labels for precision-recall calculation\n",
    "# This converts the multiclass problem into multiple binary problems\n",
    "y_true_binarized = label_binarize(y_true, classes=[1, 2])  # 'not toxic', 'ambiguous' vs 'toxic'\n",
    "y_pred_binarized = label_binarize(y_pred, classes=[1, 2])\n",
    "\n",
    "# Step 3: Compute precision, recall, and thresholds for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "\n",
    "# Only considering two classes: toxic (2) vs others (1)\n",
    "for i in range(y_true_binarized.shape[1]):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true_binarized[:, i], y_pred_binarized[:, i])\n",
    "\n",
    "# Step 4: Plot precision-recall curve for each class\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(recall[0], precision[0], marker='.', label='Not Toxic/Ambiguous vs Toxic')\n",
    "plt.plot(recall[1], precision[1], marker='.', label='Ambiguous vs Toxic')\n",
    "\n",
    "# Step 5: Customize and show the plot\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (Multiclass)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print F1 score (weighted) for multiclass\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score (weighted): {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:12<00:00, 23.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34065934065934067\n",
      "0.8157894736842105\n",
      "0.2152777777777778\n",
      "temp_label\n",
      "False    262\n",
      "True      38\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'sileod/deberta-v3-base-tasksource-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "df_normalized['temp_label'] = False  # To store the hate label\n",
    "# Create a list to store the actual hate labels (for F1 score calculation)\n",
    "true_labels =  df_normalized['toxic'] \n",
    "true_labels = df_normalized['hateful']\n",
    "true_labels = df_normalized['combined']\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate' and score >= 0.71:  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_label'] = True \n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds\n",
    "# def calculate_f1_for_threshold(df, threshold):\n",
    "#     # Predict 'True' for hate if the score is above the threshold\n",
    "#     predicted_labels = df['temp_score'].apply(lambda x: True if x < threshold else False)\n",
    "#     return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Search for the best threshold by calculating F1 score for different thresholds\n",
    "# thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "# best_threshold = 0\n",
    "# best_f1 = 0\n",
    "\n",
    "# for threshold in thresholds:\n",
    "#     f1 = calculate_f1_for_threshold(df_normalized, threshold)\n",
    "#     if f1 > best_f1:\n",
    "#         best_f1 = f1\n",
    "#         best_threshold = threshold\n",
    "\n",
    "# print(f\"Best Threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# # Apply the best threshold to label texts as hateful or not\n",
    "# df_normalized['temp'] = df_normalized['temp_score'] < best_threshold\n",
    "\n",
    "print(f1_score(true_labels, df_normalized['temp_label']))\n",
    "print(precision_score(true_labels, df_normalized['temp_label']))\n",
    "print(recall_score(true_labels, df_normalized['temp_label']))\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts\n",
    "print(df_normalized['temp_label'].value_counts())\n",
    "\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 unitary/toxic-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 63.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.4, Best F1 Score: 0.5138888888888888\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    230\n",
      "True      70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'unitary/toxic-bert'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 GroNLP/hateBERT (loves to fluctuate .-.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.4, Best F1 Score: 0.5544554455445545\n",
      "Best Threshold for hateful: 0.38, Best F1 Score: 0.3978494623655914\n",
      "Best Threshold for combined: 0.38, Best F1 Score: 0.6515837104072398\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     292\n",
      "False      8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     298\n",
      "False      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     298\n",
      "False      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'GroNLP/hateBERT'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_0':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 textdetox/xlmr-large-toxicity-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  39%|███▉      | 118/300 [00:01<00:02, 64.03it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  44%|████▍     | 132/300 [00:01<00:02, 61.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (547) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 547].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 67.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.05, Best F1 Score: 0.4936708860759494\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    216\n",
      "True      84\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'textdetox/xlmr-large-toxicity-classifier'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 facebook/roberta-hate-speech-dynabench-r4-target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  38%|███▊      | 115/300 [00:01<00:02, 64.37it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  43%|████▎     | 130/300 [00:02<00:02, 66.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 64.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.422360248447205\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    213\n",
      "True      87\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'facebook/roberta-hate-speech-dynabench-r4-target'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 cointegrated/rubert-tiny-toxicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  38%|███▊      | 115/300 [00:00<00:01, 172.11it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  50%|█████     | 150/300 [00:00<00:00, 161.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The size of tensor a (553) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:01<00:00, 155.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 1.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.96, Best F1 Score: 0.4292682926829268\n",
      "Best Threshold for combined: 1.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    169\n",
      "True     131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'cointegrated/rubert-tiny-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'non-toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x < threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] < best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  40%|███▉      | 119/300 [00:01<00:01, 101.45it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  48%|████▊     | 145/300 [00:01<00:01, 112.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:02<00:00, 104.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 1.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 1.0, Best F1 Score: 0.3914209115281501\n",
      "Best Threshold for combined: 1.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'NEITHER':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x < threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] < best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 citizenlab/distilbert-base-multilingual-cased-toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  37%|███▋      | 111/300 [00:01<00:01, 97.61it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  45%|████▌     | 135/300 [00:01<00:01, 103.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The size of tensor a (553) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:03<00:00, 98.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.5700000000000001, Best F1 Score: 0.4806201550387597\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    245\n",
      "True      55\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'citizenlab/distilbert-base-multilingual-cased-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.9 GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1 <br> \n",
    "class_labels = [\"neither\", \"offensive\", \"hate\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:03<00:00, 78.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.0, Best F1 Score: 0.39572192513368987\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_2':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.10 Hate-speech-CNERG/dehatebert-mono-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 69.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.08, Best F1 Score: 0.47619047619047616\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    206\n",
      "True      94\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'Hate-speech-CNERG/dehatebert-mono-english'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'HATE':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.11 cardiffnlp/twitter-roberta-base-hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  44%|████▍     | 132/300 [00:02<00:02, 66.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 64.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.4230769230769231\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     186\n",
      "False    114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'cardiffnlp/twitter-roberta-base-hate'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.12 Hate-speech-CNERG/bert-base-uncased-hatexplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:06<00:00, 48.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.04, Best F1 Score: 0.5714285714285714\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.4444444444444444\n",
      "Best Threshold for combined: 0.04, Best F1 Score: 0.6588235294117647\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'Hate-speech-CNERG/bert-base-uncased-hatexplain'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate speech':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.13 mrm8488/distilroberta-finetuned-tweets-hate-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  39%|███▉      | 118/300 [00:01<00:01, 116.45it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  48%|████▊     | 143/300 [00:01<00:01, 117.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:02<00:00, 107.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.04, Best F1 Score: 0.5561224489795918\n",
      "Best Threshold for hateful: 0.0, Best F1 Score: 0.3914209115281501\n",
      "Best Threshold for combined: 0.04, Best F1 Score: 0.6462264150943396\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     280\n",
      "False     20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     280\n",
      "False     20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'mrm8488/distilroberta-finetuned-tweets-hate-speech'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_0':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 testing Llama models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model='meta-llama/Llama-3.2-1B-Instruct', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:00<02:18,  2.16it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:00<00:52,  5.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:00<00:39,  7.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:00<00:31,  9.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:01<00:29,  9.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:01<00:28, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:01<00:25, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:02<00:44,  6.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:02<00:42,  6.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:02<00:36,  7.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:02<00:38,  7.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:02<00:33,  8.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:02<00:29,  9.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [00:03<00:27,  9.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [00:03<00:25, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [00:03<00:24, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [00:03<00:24, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [00:03<00:24, 11.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [00:03<00:24, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [00:04<00:24, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [00:04<00:26, 10.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [00:04<00:25, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [00:04<00:25, 10.11it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [00:04<00:24, 10.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [00:05<00:23, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [00:05<00:22, 11.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [00:05<00:21, 11.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [00:05<00:20, 11.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [00:05<00:20, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [00:05<00:21, 11.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [00:06<00:21, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [00:06<00:20, 11.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [00:06<00:20, 11.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [00:06<00:19, 12.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [00:06<00:20, 11.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [00:06<00:20, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [00:07<00:21, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [00:07<00:19, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [00:07<00:21, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [00:07<00:20, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [00:07<00:19, 11.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [00:08<00:19, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [00:08<00:19, 11.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [00:08<00:18, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [00:08<00:17, 11.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [00:08<00:18, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [00:08<00:18, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [00:09<00:17, 11.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [00:09<00:18, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [00:09<00:18, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [00:09<00:17, 11.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [00:09<00:17, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [00:09<00:17, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [00:10<00:17, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [00:10<00:16, 11.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [00:10<00:16, 11.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [00:10<00:17, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [00:10<00:16, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [00:11<00:16, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [00:11<00:16, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [00:11<00:16, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [00:11<00:17, 10.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [00:11<00:20,  8.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:12<00:20,  8.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:12<00:18,  9.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:12<00:16, 10.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:12<00:16, 10.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:12<00:14, 11.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [00:13<00:14, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:13<00:15, 10.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [00:13<00:15, 10.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [00:13<00:14, 11.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [00:13<00:13, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [00:13<00:13, 11.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [00:14<00:13, 11.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [00:14<00:13, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [00:14<00:13, 11.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [00:14<00:12, 11.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [00:14<00:12, 11.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [00:14<00:11, 12.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [00:15<00:11, 12.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [00:15<00:11, 12.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [00:15<00:11, 12.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [00:15<00:11, 11.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [00:15<00:11, 11.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [00:15<00:10, 12.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [00:16<00:11, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [00:16<00:11, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [00:16<00:10, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [00:16<00:10, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [00:16<00:10, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [00:16<00:10, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [00:17<00:09, 11.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [00:17<00:09, 12.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [00:17<00:09, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [00:17<00:09, 11.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [00:17<00:08, 12.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [00:17<00:08, 12.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [00:18<00:08, 12.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [00:18<00:08, 12.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [00:18<00:08, 11.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [00:18<00:08, 12.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [00:18<00:09, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [00:19<00:09, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [00:19<00:08, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [00:19<00:08, 10.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [00:19<00:07, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [00:19<00:07, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [00:19<00:07, 11.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [00:20<00:07, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [00:20<00:07, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [00:20<00:06, 11.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [00:20<00:06, 12.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [00:20<00:06, 12.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [00:20<00:06, 11.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [00:21<00:06, 10.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [00:21<00:06, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [00:21<00:06, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [00:21<00:05, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [00:21<00:05, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [00:21<00:05, 12.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [00:22<00:05, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [00:22<00:05, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [00:22<00:05, 10.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [00:22<00:04, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [00:22<00:04, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [00:23<00:04, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [00:23<00:04, 10.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [00:23<00:04, 10.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [00:23<00:04, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [00:23<00:03, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [00:24<00:03, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [00:24<00:03, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [00:24<00:03, 10.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [00:24<00:03, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [00:24<00:02, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [00:24<00:02, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [00:25<00:02, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [00:25<00:02, 10.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [00:25<00:02, 10.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:25<00:01, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:25<00:01, 11.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:26<00:01, 11.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:26<00:01, 12.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:26<00:01, 11.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:26<00:00, 11.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:26<00:00, 11.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:26<00:00, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:27<00:00, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:27<00:00, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:27<00:00, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:27<00:00, 10.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic or hateful. Please respond with only False (not toxic or hateful) or True (very toxic or hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_combined'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_combined'] = df_normalized['Llama_combined'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:00<00:24, 12.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:00<00:27, 10.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:00<00:29, 10.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:00<00:27, 10.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:00<00:25, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:01<00:25, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:01<00:24, 11.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:01<00:39,  7.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:01<00:37,  7.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:02<00:36,  7.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:02<00:35,  7.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:02<00:32,  8.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:02<00:28,  9.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:02<00:28,  9.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:02<00:27,  9.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [00:03<00:25, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [00:03<00:25, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [00:03<00:25, 10.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [00:03<00:24, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [00:03<00:27,  9.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [00:04<00:25, 10.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [00:04<00:26,  9.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [00:04<00:26,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [00:04<00:25, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [00:04<00:24, 10.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [00:04<00:22, 11.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [00:05<00:21, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [00:05<00:22, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [00:05<00:22, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [00:05<00:21, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [00:05<00:21, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [00:06<00:20, 11.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [00:06<00:20, 11.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [00:06<00:19, 11.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [00:06<00:19, 12.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [00:06<00:18, 12.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [00:06<00:18, 12.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [00:06<00:18, 12.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [00:07<00:18, 12.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [00:07<00:18, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [00:07<00:19, 11.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [00:07<00:19, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [00:07<00:18, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [00:08<00:18, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [00:08<00:18, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [00:08<00:17, 11.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [00:08<00:18, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [00:08<00:17, 11.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [00:08<00:18, 11.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [00:09<00:18, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [00:09<00:18, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [00:09<00:17, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [00:09<00:18, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [00:09<00:18, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [00:10<00:17, 11.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [00:10<00:16, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [00:10<00:16, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [00:10<00:15, 11.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [00:10<00:15, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [00:10<00:16, 11.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [00:11<00:16, 11.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [00:11<00:16, 10.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [00:11<00:19,  8.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:11<00:21,  8.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:12<00:19,  9.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:12<00:17,  9.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:12<00:16, 10.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:12<00:15, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [00:12<00:15, 10.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:12<00:15, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [00:13<00:14, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [00:13<00:14, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [00:13<00:13, 11.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [00:13<00:12, 12.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [00:13<00:12, 12.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [00:13<00:12, 11.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [00:14<00:13, 11.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [00:14<00:12, 11.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [00:14<00:12, 12.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [00:14<00:11, 11.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [00:14<00:11, 12.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [00:14<00:11, 12.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [00:15<00:10, 12.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [00:15<00:10, 12.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [00:15<00:10, 12.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [00:15<00:10, 11.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [00:15<00:10, 12.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [00:15<00:10, 12.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [00:16<00:10, 12.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [00:16<00:10, 12.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [00:16<00:10, 12.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [00:16<00:10, 11.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [00:16<00:09, 11.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [00:16<00:10, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [00:17<00:10, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [00:17<00:09, 11.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [00:17<00:09, 11.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [00:17<00:09, 11.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [00:17<00:08, 11.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [00:17<00:08, 12.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [00:18<00:08, 12.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [00:18<00:07, 12.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [00:18<00:08, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [00:18<00:08, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [00:18<00:08, 10.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [00:19<00:08, 10.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [00:19<00:08, 10.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [00:19<00:08, 10.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [00:19<00:07, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [00:19<00:07, 10.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [00:19<00:07, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [00:20<00:07, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [00:20<00:06, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [00:20<00:06, 11.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [00:20<00:06, 11.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [00:20<00:06, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [00:21<00:05, 11.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [00:21<00:05, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [00:21<00:05, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [00:21<00:05, 11.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [00:21<00:05, 11.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [00:21<00:05, 10.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [00:22<00:05,  9.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [00:22<00:05,  9.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [00:22<00:05, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [00:22<00:04, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [00:22<00:04, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [00:23<00:04,  9.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [00:23<00:04, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [00:23<00:03, 10.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [00:23<00:03, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [00:23<00:03, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [00:24<00:03, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [00:24<00:03, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [00:24<00:03, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [00:24<00:02, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [00:24<00:02, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [00:24<00:02, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [00:25<00:02, 10.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [00:25<00:02, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:25<00:01, 10.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:25<00:01, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:25<00:01, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:25<00:01, 11.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:26<00:01, 11.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:26<00:00, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:26<00:00, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:26<00:00, 11.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:26<00:00, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:27<00:00, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:27<00:00, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:27<00:00, 10.97it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic. Please respond with only False (not toxic) or True (very toxic) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_toxic'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_toxic'] = df_normalized['Llama_toxic'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:00<00:26, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:00<00:24, 11.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:00<00:25, 11.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:00<00:25, 11.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:00<00:25, 11.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:01<00:24, 11.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:01<00:23, 12.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:01<00:24, 11.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:01<00:23, 11.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:01<00:28,  9.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:01<00:26, 10.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:02<00:24, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:02<00:24, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:02<00:23, 11.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [00:02<00:22, 11.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [00:02<00:23, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [00:03<00:24, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [00:03<00:23, 11.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [00:03<00:25, 10.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [00:03<00:24, 10.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [00:03<00:23, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [00:03<00:22, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [00:04<00:21, 11.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [00:04<00:20, 12.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [00:04<00:19, 12.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [00:04<00:19, 12.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [00:04<00:20, 11.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [00:04<00:20, 12.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [00:05<00:21, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [00:05<00:20, 11.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [00:05<00:20, 11.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [00:05<00:20, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [00:05<00:20, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [00:05<00:19, 11.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [00:06<00:19, 11.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [00:06<00:20, 11.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [00:06<00:19, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [00:06<00:19, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [00:06<00:20, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [00:07<00:19, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [00:07<00:19, 11.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [00:07<00:18, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [00:07<00:18, 11.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [00:07<00:17, 11.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [00:07<00:17, 11.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [00:08<00:18, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [00:08<00:18, 11.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [00:08<00:18, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [00:08<00:18, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [00:08<00:18, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [00:08<00:18, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [00:09<00:18, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [00:09<00:17, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [00:09<00:17, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [00:09<00:16, 11.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [00:09<00:16, 11.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [00:09<00:15, 11.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [00:10<00:16, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [00:10<00:16, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [00:10<00:17, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [00:10<00:20,  8.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [00:11<00:19,  9.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:11<00:21,  8.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:11<00:18,  9.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:11<00:17,  9.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:11<00:15, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:11<00:14, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [00:12<00:15, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:12<00:15, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [00:12<00:14, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [00:12<00:13, 11.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [00:12<00:12, 12.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [00:12<00:12, 12.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [00:13<00:12, 12.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [00:13<00:13, 11.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [00:13<00:13, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [00:13<00:13, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [00:13<00:12, 11.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [00:14<00:12, 11.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [00:14<00:12, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [00:14<00:11, 11.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [00:14<00:11, 12.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [00:14<00:11, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [00:14<00:11, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [00:15<00:11, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [00:15<00:11, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [00:15<00:10, 11.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [00:15<00:10, 11.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [00:15<00:10, 11.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [00:15<00:10, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [00:16<00:09, 12.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [00:16<00:09, 12.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [00:16<00:09, 12.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [00:16<00:09, 12.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [00:16<00:08, 12.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [00:16<00:08, 12.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [00:17<00:08, 12.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [00:17<00:08, 12.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [00:17<00:08, 12.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [00:17<00:08, 12.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [00:17<00:08, 11.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [00:17<00:09,  9.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [00:18<00:09,  9.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [00:18<00:09,  9.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [00:18<00:09,  9.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [00:18<00:08, 10.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [00:18<00:08, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [00:19<00:07, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [00:19<00:07, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [00:19<00:07, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [00:19<00:06, 11.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [00:19<00:06, 11.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [00:19<00:06, 11.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [00:20<00:06, 12.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [00:20<00:06, 11.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [00:20<00:05, 11.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [00:20<00:05, 12.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [00:20<00:05, 12.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [00:20<00:04, 12.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [00:21<00:04, 12.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [00:21<00:04, 12.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [00:21<00:05, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [00:21<00:04, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [00:21<00:04, 11.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [00:21<00:04, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [00:22<00:04, 11.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [00:22<00:04, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [00:22<00:04, 10.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [00:22<00:03, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [00:22<00:03, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [00:23<00:03, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [00:23<00:03, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [00:23<00:03, 11.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [00:23<00:02, 11.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [00:23<00:02, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [00:23<00:02, 10.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [00:24<00:02, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [00:24<00:02,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [00:24<00:02, 10.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:24<00:01, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:24<00:01, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:25<00:01, 11.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:25<00:01, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:25<00:01, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:25<00:01, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:25<00:00, 11.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:25<00:00, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:26<00:00, 10.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:26<00:00, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:26<00:00, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:26<00:00, 11.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is hateful. Please respond with only False (not hateful) or True (very hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_hate'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_hate'] = df_normalized['Llama_hate'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama_combined\n",
      "False    200\n",
      "True     100\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.4309707128384601\n",
      "Llama_toxic\n",
      "False    280\n",
      "True      20\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.4832530432530433\n",
      "Llama_hate\n",
      "False    259\n",
      "True      41\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.6634645151650979\n"
     ]
    }
   ],
   "source": [
    "print(df_normalized['Llama_combined'].value_counts())\n",
    "# print(df_normalized['combined'].value_counts())\n",
    "f1 = f1_score(df_normalized['combined'], df_normalized['Llama_combined'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_toxic'].value_counts())\n",
    "f1 = f1_score(df_normalized['toxic'], df_normalized['Llama_toxic'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_hate'].value_counts())\n",
    "f1 = f1_score(df_normalized['hateful'], df_normalized['Llama_hate'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model='meta-llama/Llama-3.2-3B-Instruct', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:02<13:52,  2.79s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:04<11:50,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:07<11:14,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:09<10:57,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:11<10:53,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:13<10:40,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:15<10:34,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:17<10:36,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:19<10:25,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:22<10:19,  2.14s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:24<10:14,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:26<10:09,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:28<10:03,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:30<10:00,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:32<09:52,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:34<10:05,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:36<09:52,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:38<09:46,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:40<10:00,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:43<09:52,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:45<09:50,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:47<09:59,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:49<09:46,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:51<09:37,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [00:53<09:34,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:55<09:29,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [00:57<09:21,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:59<09:23,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [01:01<09:19,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [01:03<09:20,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [01:06<09:25,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [01:08<09:18,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [01:10<09:16,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [01:12<09:18,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [01:14<09:15,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [01:16<09:10,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [01:18<09:06,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [01:20<09:16,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [01:22<09:11,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [01:24<09:04,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [01:26<09:01,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [01:29<08:55,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [01:31<08:47,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [01:33<08:43,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [01:35<08:42,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [01:37<08:43,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [01:39<08:38,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [01:41<08:34,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [01:43<08:36,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [01:45<08:33,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [01:47<08:31,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [01:49<08:27,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [01:51<08:28,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [01:53<08:27,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [01:55<08:23,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [01:57<08:18,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [01:59<08:18,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [02:01<08:14,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [02:03<08:11,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [02:05<08:07,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [02:07<08:07,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [02:09<08:05,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [02:11<08:04,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [02:14<08:05,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [02:16<08:02,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [02:18<07:59,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [02:20<07:55,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [02:22<07:52,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [02:24<07:51,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [02:26<07:52,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [02:28<07:54,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [02:30<07:48,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [02:32<07:43,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [02:34<07:44,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [02:36<07:42,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [02:38<07:39,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [02:40<07:35,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [02:42<07:32,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [02:44<07:28,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [02:46<07:24,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [02:48<07:25,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [02:50<07:21,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [02:52<07:21,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [02:54<07:20,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [02:56<07:21,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [02:58<07:17,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [03:00<07:12,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [03:03<07:11,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [03:05<07:08,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [03:07<07:08,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [03:09<07:10,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [03:11<07:05,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [03:13<07:03,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [03:15<06:58,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [03:17<07:03,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [03:19<07:01,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [03:21<06:55,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [03:23<06:53,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [03:25<07:04,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [03:28<07:20,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [03:30<07:20,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [03:32<07:27,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [03:35<07:21,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [03:37<07:16,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [03:39<07:09,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [03:41<07:10,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [03:43<07:09,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [03:46<07:07,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [03:48<07:04,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [03:50<07:06,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [03:52<06:57,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [03:54<06:48,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [03:57<06:54,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [03:59<06:46,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [04:01<06:50,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [04:03<06:38,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [04:05<06:28,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [04:07<06:20,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [04:09<06:14,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [04:11<06:17,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [04:13<06:10,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [04:16<06:25,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [04:18<06:14,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [04:20<06:07,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [04:22<06:16,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 126/300 [04:24<06:08,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [04:26<06:02,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 128/300 [04:28<05:56,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [04:30<05:51,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 130/300 [04:32<05:46,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [04:34<05:41,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 132/300 [04:36<05:43,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [04:38<05:39,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▍     | 134/300 [04:40<05:47,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [04:42<05:41,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 136/300 [04:45<05:43,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [04:47<05:43,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 138/300 [04:49<05:40,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [04:51<05:35,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 140/300 [04:53<05:32,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [04:55<05:27,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 142/300 [04:57<05:36,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [04:59<05:35,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 144/300 [05:02<05:36,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [05:04<05:38,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▊     | 146/300 [05:06<05:36,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [05:08<05:32,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 148/300 [05:10<05:31,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [05:13<05:35,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 150/300 [05:15<05:32,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [05:17<05:28,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 152/300 [05:19<05:28,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [05:22<05:33,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████▏    | 154/300 [05:24<05:25,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [05:26<05:23,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 156/300 [05:28<05:18,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [05:30<05:12,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 158/300 [05:33<05:13,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [05:35<05:12,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 160/300 [05:37<05:18,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [05:40<05:16,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 162/300 [05:42<05:09,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [05:44<05:08,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▍    | 164/300 [05:46<05:08,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [05:49<05:10,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 166/300 [05:51<05:04,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [05:53<04:58,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 168/300 [05:55<04:49,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [05:57<04:40,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 170/300 [05:59<04:33,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [06:01<04:27,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 172/300 [06:03<04:23,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [06:05<04:21,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 174/300 [06:07<04:17,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [06:09<04:13,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▊    | 176/300 [06:11<04:12,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [06:13<04:10,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 178/300 [06:15<04:07,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [06:17<04:04,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 180/300 [06:19<04:01,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [06:21<04:01,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 182/300 [06:23<03:59,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [06:25<03:56,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████▏   | 184/300 [06:28<03:54,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [06:30<03:53,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 186/300 [06:32<03:51,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [06:34<03:48,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 188/300 [06:36<03:58,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [06:38<03:58,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 190/300 [06:40<03:55,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [06:42<03:49,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 192/300 [06:44<03:46,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [06:46<03:44,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▍   | 194/300 [06:49<03:45,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [06:51<03:45,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 196/300 [06:53<03:41,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [06:55<03:36,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 198/300 [06:57<03:32,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [06:59<03:28,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 200/300 [07:01<03:25,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [07:03<03:22,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 202/300 [07:05<03:23,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [07:07<03:25,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 204/300 [07:10<03:22,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [07:12<03:17,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▊   | 206/300 [07:14<03:14,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [07:16<03:15,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 208/300 [07:18<03:11,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [07:20<03:07,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 210/300 [07:22<03:05,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [07:24<03:02,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 212/300 [07:26<03:01,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [07:28<02:59,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████▏  | 214/300 [07:30<02:56,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [07:32<02:53,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 216/300 [07:34<02:52,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [07:36<02:50,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 218/300 [07:38<02:50,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [07:41<02:48,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 220/300 [07:43<02:46,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [07:45<02:42,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 222/300 [07:47<02:41,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [07:49<02:38,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▍  | 224/300 [07:51<02:35,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [07:53<02:33,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 226/300 [07:55<02:31,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [07:57<02:29,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 228/300 [07:59<02:28,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [08:01<02:27,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 230/300 [08:03<02:24,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [08:05<02:23,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 232/300 [08:07<02:21,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [08:09<02:18,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 234/300 [08:11<02:16,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [09:00<17:23, 16.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▊  | 236/300 [09:02<12:39, 11.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [09:04<09:21,  8.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 238/300 [09:06<07:05,  6.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [09:08<05:30,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 240/300 [09:10<04:24,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [09:13<03:40,  3.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 242/300 [09:15<03:09,  3.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [09:17<02:44,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████▏ | 244/300 [09:19<02:26,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [09:21<02:14,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 246/300 [09:23<02:06,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [09:25<01:59,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 248/300 [09:27<01:53,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [09:29<01:48,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 250/300 [09:31<01:44,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [09:33<01:41,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 252/300 [09:35<01:41,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [09:37<01:38,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▍ | 254/300 [09:39<01:36,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [09:41<01:33,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 256/300 [09:43<01:30,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [09:45<01:28,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 258/300 [09:48<01:26,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [09:50<01:24,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 260/300 [09:52<01:22,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [09:54<01:20,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 262/300 [09:56<01:19,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [09:58<01:17,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 264/300 [10:00<01:16,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [10:02<01:12,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▊ | 266/300 [10:04<01:10,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [10:06<01:08,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 268/300 [10:08<01:06,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [10:11<01:07,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 270/300 [10:13<01:05,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [10:15<01:01,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 272/300 [10:17<00:58,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [10:19<00:55,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████▏| 274/300 [10:21<00:54,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [10:24<00:54,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 276/300 [10:26<00:53,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [10:28<00:50,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 278/300 [10:30<00:48,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [10:32<00:46,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 280/300 [10:35<00:44,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [10:37<00:42,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 282/300 [10:39<00:40,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [10:41<00:37,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▍| 284/300 [10:44<00:35,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [10:46<00:33,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 286/300 [10:48<00:31,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [10:50<00:28,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 288/300 [10:52<00:26,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [10:55<00:24,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 290/300 [10:57<00:21,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [10:59<00:19,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 292/300 [11:01<00:17,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [11:03<00:15,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 294/300 [11:06<00:13,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [11:08<00:10,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▊| 296/300 [11:10<00:08,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [11:12<00:06,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 298/300 [11:14<00:04,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [11:16<00:02,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [11:18<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic or hateful. Please respond with only False (not toxic or hateful) or True (very toxic or hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_combined'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_combined'] = df_normalized['Llama_combined'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:02<10:58,  2.20s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:04<10:47,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:06<10:53,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:08<10:47,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:11<11:06,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:13<11:10,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:15<10:51,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:17<10:38,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:19<10:23,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:21<10:17,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:23<10:13,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:25<10:05,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:28<10:04,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:30<09:59,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:32<09:56,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:34<10:07,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:36<10:03,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:38<09:59,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:41<10:13,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:43<10:05,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:45<09:56,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:47<09:50,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:49<09:39,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:51<09:37,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [00:53<09:36,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:55<09:31,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [00:57<09:29,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:59<09:25,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [01:01<09:25,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [01:03<09:25,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [01:06<09:23,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [01:08<09:21,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [01:10<09:21,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [01:12<09:20,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [01:14<09:21,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [01:16<09:24,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [01:18<09:15,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [01:21<09:29,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [01:23<09:21,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [01:25<09:15,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [01:27<09:15,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [01:29<09:04,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [01:31<09:07,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [01:33<09:06,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [01:35<09:00,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [01:37<08:50,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [01:39<08:43,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [01:42<08:40,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [01:44<08:37,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [01:46<08:37,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [01:48<08:36,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [01:50<08:37,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [01:52<08:31,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [01:54<08:28,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [01:56<08:28,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [01:58<08:26,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [02:00<08:27,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [02:02<08:24,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [02:04<08:24,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [02:07<08:23,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [02:09<08:19,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [02:11<08:12,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [02:13<08:12,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [02:15<08:08,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [02:17<08:09,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [02:19<08:07,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [02:21<08:01,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [02:23<07:59,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [02:25<07:54,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [02:27<07:53,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [02:29<07:55,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [02:31<07:53,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [02:34<07:56,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [02:36<07:56,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [02:38<07:54,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [02:40<07:51,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [02:42<07:50,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [02:44<07:50,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [02:46<07:46,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [02:48<07:44,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [02:50<07:42,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [02:53<07:40,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [02:55<07:35,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [02:57<07:30,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [02:59<07:30,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [03:01<07:28,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [03:03<07:24,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [03:05<07:24,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [03:07<07:20,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [03:09<07:19,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [03:11<07:17,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [03:13<07:12,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [03:15<07:10,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [03:18<07:08,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [03:20<07:12,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [03:22<07:08,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [03:24<07:05,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [03:26<06:59,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [03:28<06:55,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [03:30<06:56,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [03:32<06:55,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [03:34<07:00,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [03:36<06:54,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [03:39<06:53,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [03:41<06:47,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [03:43<06:44,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [03:45<06:40,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [03:47<06:37,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [03:49<06:40,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [03:51<06:35,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [03:53<06:32,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [03:55<06:30,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [03:57<06:27,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [03:59<06:23,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [04:01<06:27,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [04:04<06:27,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [04:06<06:24,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [04:08<06:20,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [04:10<06:17,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [04:12<06:19,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [04:14<06:15,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [04:16<06:32,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [04:19<06:25,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [04:21<06:18,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [04:23<06:27,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 126/300 [04:25<06:18,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [04:27<06:12,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 128/300 [04:29<06:06,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [04:31<05:58,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 130/300 [04:33<05:56,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [04:35<05:53,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 132/300 [04:38<05:48,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [04:40<05:47,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▍     | 134/300 [04:42<05:54,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [04:44<05:48,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 136/300 [04:46<05:47,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [04:48<05:44,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 138/300 [04:50<05:42,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [04:52<05:36,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 140/300 [04:54<05:35,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [04:57<05:33,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 142/300 [04:59<05:30,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [05:01<05:26,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 144/300 [05:03<05:25,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [05:05<05:21,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▊     | 146/300 [05:07<05:17,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [05:09<05:16,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 148/300 [05:11<05:16,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [05:13<05:14,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 150/300 [05:15<05:11,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [05:17<05:14,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 152/300 [05:19<05:10,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [05:22<05:09,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████▏    | 154/300 [05:24<05:06,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [05:26<05:01,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 156/300 [05:28<04:59,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [05:30<04:57,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 158/300 [05:32<04:57,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [05:34<04:56,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 160/300 [05:36<04:52,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [05:38<04:52,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 162/300 [05:40<04:49,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [05:42<04:44,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▍    | 164/300 [05:45<04:43,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [05:47<04:42,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 166/300 [05:49<04:39,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [05:51<04:37,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 168/300 [05:53<04:36,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [05:55<04:32,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 170/300 [05:57<04:31,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [05:59<04:28,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 172/300 [06:01<04:25,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [06:03<04:24,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 174/300 [06:05<04:21,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [06:07<04:19,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▊    | 176/300 [06:10<04:19,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [06:12<04:16,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 178/300 [06:14<04:13,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [06:16<04:09,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 180/300 [06:18<04:09,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [06:20<04:07,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 182/300 [06:22<04:04,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [06:24<04:02,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████▏   | 184/300 [06:26<03:59,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [06:28<03:56,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 186/300 [06:30<03:57,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [06:32<03:54,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 188/300 [06:34<03:53,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [06:36<03:50,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 190/300 [06:39<03:48,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [06:41<03:45,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 192/300 [06:43<03:43,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [06:45<03:41,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▍   | 194/300 [06:47<03:42,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [06:49<03:41,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 196/300 [06:51<03:37,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [06:53<03:34,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 198/300 [06:55<03:31,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [06:57<03:29,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 200/300 [06:59<03:27,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [07:01<03:24,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 202/300 [07:04<03:24,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [07:06<03:27,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 204/300 [07:08<03:23,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [07:10<03:21,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▊   | 206/300 [07:12<03:17,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [07:14<03:17,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 208/300 [07:16<03:13,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [07:18<03:11,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 210/300 [07:21<03:09,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [07:23<03:06,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 212/300 [07:25<03:06,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [07:27<03:04,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████▏  | 214/300 [07:29<03:00,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [07:31<02:59,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 216/300 [07:33<02:56,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [07:35<02:53,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 218/300 [07:37<02:52,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [07:39<02:50,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 220/300 [07:42<02:48,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [07:44<02:45,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 222/300 [07:46<02:43,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [07:48<02:41,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▍  | 224/300 [07:50<02:38,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [07:52<02:36,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 226/300 [07:54<02:34,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [07:56<02:31,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 228/300 [07:58<02:29,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [08:00<02:27,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 230/300 [08:02<02:27,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [08:05<02:25,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 232/300 [08:07<02:22,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [08:09<02:18,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 234/300 [08:11<02:16,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [09:00<17:40, 16.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▊  | 236/300 [09:02<12:51, 12.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [09:05<09:31,  9.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 238/300 [09:07<07:11,  6.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [09:09<05:35,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 240/300 [09:11<04:28,  4.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [09:13<03:43,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 242/300 [09:15<03:13,  3.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [09:17<02:48,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████▏ | 244/300 [09:19<02:31,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [09:21<02:18,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 246/300 [09:24<02:09,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [09:26<02:01,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 248/300 [09:28<01:56,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [09:30<01:52,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 250/300 [09:32<01:48,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [09:34<01:44,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 252/300 [09:36<01:44,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [09:38<01:40,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▍ | 254/300 [09:40<01:36,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [09:42<01:33,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 256/300 [09:44<01:30,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [09:47<01:29,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 258/300 [09:49<01:27,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [09:51<01:25,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 260/300 [09:53<01:23,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [09:55<01:21,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 262/300 [09:57<01:19,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [09:59<01:18,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 264/300 [10:01<01:16,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [10:03<01:14,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▊ | 266/300 [10:05<01:11,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [10:08<01:08,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 268/300 [10:10<01:06,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [10:12<01:04,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 270/300 [10:14<01:02,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [10:16<01:00,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 272/300 [10:18<00:57,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [10:20<00:55,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████▏| 274/300 [10:22<00:54,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [10:24<00:53,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 276/300 [10:26<00:50,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [10:28<00:48,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 278/300 [10:30<00:45,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [10:33<00:43,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 280/300 [10:35<00:41,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [10:37<00:39,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 282/300 [10:39<00:37,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [10:41<00:35,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▍| 284/300 [10:43<00:33,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [10:45<00:31,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 286/300 [10:47<00:29,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [10:49<00:27,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 288/300 [10:51<00:24,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [10:53<00:22,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 290/300 [10:55<00:20,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [10:57<00:18,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 292/300 [11:00<00:16,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [11:02<00:14,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 294/300 [11:04<00:12,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [11:06<00:10,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▊| 296/300 [11:08<00:08,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [11:10<00:06,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 298/300 [11:12<00:04,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [11:14<00:02,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [11:16<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic. Please respond with only False (not toxic) or True (very toxic) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_toxic'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_toxic'] = df_normalized['Llama_toxic'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:02<11:05,  2.23s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:04<10:48,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:06<10:54,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:08<10:46,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:11<10:56,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:13<10:43,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:15<10:37,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:17<10:33,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:19<10:21,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:21<10:19,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:23<10:20,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:25<10:14,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:28<10:07,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:30<10:04,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:32<10:02,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:34<10:13,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:36<10:05,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:38<10:02,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:41<10:19,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:43<10:09,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:45<09:57,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:47<09:57,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:49<09:49,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:51<09:41,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [00:53<09:44,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:55<09:39,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [00:57<09:33,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [01:00<09:38,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [01:02<09:37,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [01:04<09:28,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [01:06<09:29,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [01:08<09:26,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [01:10<09:29,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [01:12<09:30,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [01:14<09:24,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [01:17<09:25,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [01:19<09:20,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [01:21<09:33,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [01:23<09:28,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [01:26<09:33,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [01:28<09:32,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [01:30<09:34,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [01:32<09:22,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [01:34<09:12,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [01:36<09:05,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [01:38<08:58,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [01:40<08:51,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [01:42<08:45,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [01:45<08:47,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [01:47<08:49,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [01:49<08:42,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [01:51<08:35,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [01:53<08:28,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [01:55<08:25,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [01:57<08:24,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [01:59<08:21,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [02:01<08:28,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [02:03<08:28,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [02:05<08:24,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [02:07<08:20,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [02:10<08:17,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [02:12<08:12,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [02:14<08:23,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [02:16<08:28,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [02:18<08:21,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [02:20<08:21,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [02:22<08:13,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [02:24<08:05,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [02:26<08:01,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [02:29<07:58,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [02:31<07:59,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [02:33<07:53,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [02:35<07:49,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [02:37<07:50,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [02:39<07:45,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [02:41<07:41,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [02:43<08:09,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [02:46<08:19,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [02:48<07:59,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [02:50<07:47,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [02:52<07:42,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [02:54<07:35,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [02:56<07:29,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [02:58<07:27,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [03:00<07:26,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [03:02<07:27,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [03:04<07:21,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [03:06<07:15,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [03:08<07:12,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [03:10<07:14,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [03:13<07:15,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [03:15<07:08,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [03:17<07:10,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [03:19<07:08,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [03:21<07:11,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [03:23<07:05,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [03:25<07:00,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [03:27<06:53,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [03:29<06:51,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [03:31<06:49,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [03:33<06:43,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [03:35<06:51,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [03:37<06:46,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [03:39<06:40,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [03:41<06:38,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [03:43<06:34,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [03:45<06:35,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [03:48<06:34,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [03:50<06:30,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [03:52<06:27,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [03:54<06:23,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [03:56<06:21,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [03:58<06:20,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [04:00<06:15,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [04:02<06:19,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [04:04<06:17,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [04:06<06:18,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [04:08<06:16,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [04:10<06:14,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [04:12<06:16,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [04:14<06:13,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [04:17<06:26,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [04:19<06:17,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [04:21<06:08,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [04:23<06:18,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 126/300 [04:25<06:09,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [04:27<06:03,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 128/300 [04:29<05:58,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [04:31<05:55,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 130/300 [04:33<05:51,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [04:35<05:46,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 132/300 [04:37<05:43,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [04:39<05:38,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▍     | 134/300 [04:41<05:44,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [04:44<05:43,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 136/300 [04:46<05:40,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [04:48<05:39,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 138/300 [04:50<05:34,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [04:52<05:30,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 140/300 [04:54<05:27,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [04:56<05:22,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 142/300 [04:58<05:20,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [05:00<05:18,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 144/300 [05:02<05:18,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [05:04<05:16,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▊     | 146/300 [05:06<05:13,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [05:08<05:09,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 148/300 [05:10<05:06,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [05:12<05:06,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 150/300 [05:14<05:08,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [05:16<05:06,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 152/300 [05:18<05:04,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [05:20<05:02,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████▏    | 154/300 [05:22<05:01,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [05:24<04:58,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 156/300 [05:27<04:56,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [05:29<04:55,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 158/300 [05:31<04:52,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [05:33<04:50,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 160/300 [05:35<04:47,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [05:37<04:43,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 162/300 [05:39<04:40,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [05:41<04:38,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▍    | 164/300 [05:43<04:39,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [05:45<04:37,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 166/300 [05:47<04:36,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [05:49<04:34,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 168/300 [05:51<04:34,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [05:53<04:31,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 170/300 [05:55<04:29,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [05:57<04:25,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 172/300 [05:59<04:23,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [06:02<04:21,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 174/300 [06:04<04:17,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [06:06<04:15,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▊    | 176/300 [06:08<04:13,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [06:10<04:14,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 178/300 [06:12<04:12,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [06:14<04:09,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 180/300 [06:16<04:08,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [06:18<04:07,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 182/300 [06:20<04:05,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [06:22<04:01,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████▏   | 184/300 [06:24<04:00,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [06:26<03:57,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 186/300 [06:28<03:55,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [06:30<03:52,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 188/300 [06:32<03:50,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [06:35<03:48,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 190/300 [06:37<03:47,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [06:39<03:43,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 192/300 [06:41<03:42,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [06:43<03:39,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▍   | 194/300 [06:45<03:39,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [06:47<03:37,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 196/300 [06:49<03:35,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [06:51<03:32,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 198/300 [06:53<03:30,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [06:55<03:27,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 200/300 [06:57<03:23,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [06:59<03:22,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 202/300 [07:01<03:23,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [07:04<03:25,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 204/300 [07:06<03:23,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [07:08<03:18,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▊   | 206/300 [07:10<03:15,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [07:12<03:15,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 208/300 [07:14<03:11,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [07:16<03:08,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 210/300 [07:18<03:07,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [07:20<03:04,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 212/300 [07:22<03:03,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [07:24<03:00,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████▏  | 214/300 [07:26<02:57,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [07:28<02:55,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 216/300 [07:31<02:54,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [07:33<02:50,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 218/300 [07:35<02:48,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [07:37<02:45,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 220/300 [07:39<02:43,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [07:41<02:41,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 222/300 [07:43<02:38,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [07:45<02:36,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▍  | 224/300 [07:47<02:36,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [07:49<02:35,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 226/300 [07:51<02:33,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [07:53<02:30,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 228/300 [07:55<02:27,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [07:57<02:25,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 230/300 [07:59<02:24,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [08:01<02:23,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 232/300 [08:03<02:21,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [08:05<02:18,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 234/300 [08:08<02:16,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [08:56<17:18, 15.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▊  | 236/300 [08:58<12:34, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [09:00<09:17,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 238/300 [09:02<07:02,  6.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [09:04<05:29,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 240/300 [09:06<04:23,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [09:08<03:39,  3.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 242/300 [09:11<03:10,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [09:13<02:45,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████▏ | 244/300 [09:15<02:28,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [09:17<02:16,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 246/300 [09:19<02:07,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [09:21<02:00,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 248/300 [09:23<01:56,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [09:25<01:53,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 250/300 [09:27<01:49,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [09:29<01:45,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 252/300 [09:32<01:44,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [09:34<01:40,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▍ | 254/300 [09:36<01:38,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [09:38<01:34,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 256/300 [09:40<01:31,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [09:42<01:29,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 258/300 [09:44<01:26,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [09:46<01:25,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 260/300 [09:48<01:23,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [09:50<01:20,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 262/300 [09:52<01:18,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [09:54<01:16,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 264/300 [09:56<01:14,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [09:59<01:12,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▊ | 266/300 [10:01<01:10,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [10:03<01:08,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 268/300 [10:05<01:06,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [10:07<01:04,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 270/300 [10:09<01:02,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [10:11<00:59,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 272/300 [10:13<00:57,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [10:15<00:55,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████▏| 274/300 [10:17<00:53,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [10:19<00:52,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 276/300 [10:21<00:50,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [10:23<00:47,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 278/300 [10:26<00:45,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [10:28<00:43,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 280/300 [10:30<00:41,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [10:32<00:39,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 282/300 [10:34<00:37,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [10:36<00:35,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▍| 284/300 [10:38<00:32,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [10:40<00:30,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 286/300 [10:42<00:28,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [10:44<00:26,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 288/300 [10:46<00:24,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [10:48<00:22,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 290/300 [10:50<00:20,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [10:52<00:18,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 292/300 [10:54<00:16,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [10:57<00:14,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 294/300 [10:59<00:12,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [11:01<00:10,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▊| 296/300 [11:03<00:08,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [11:05<00:06,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 298/300 [11:07<00:04,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [11:09<00:02,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [11:11<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is hateful. Please respond with only False (not hateful) or True (very hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_hate'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_hate'] = df_normalized['Llama_hate'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama_combined\n",
      "False    219\n",
      "True      81\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.60224\n",
      "Llama_toxic\n",
      "False    202\n",
      "True      98\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.6344615384615384\n",
      "Llama_hate\n",
      "False    253\n",
      "True      47\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.7363759439143762\n"
     ]
    }
   ],
   "source": [
    "print(df_normalized['Llama_combined'].value_counts())\n",
    "# print(df_normalized['combined'].value_counts())\n",
    "f1 = f1_score(df_normalized['combined'], df_normalized['Llama_combined'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_toxic'].value_counts())\n",
    "f1 = f1_score(df_normalized['toxic'], df_normalized['Llama_toxic'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_hate'].value_counts())\n",
    "f1 = f1_score(df_normalized['hateful'], df_normalized['Llama_hate'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.93s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model='aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:30<2:29:55, 30.08s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [02:18<53:01, 10.97s/it] You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [51:51<00:00, 10.37s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic or hateful. Please respond with only False (not toxic or hateful) or True (very toxic or hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_combined'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_combined'] = df_normalized['Llama_combined'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:10<52:45, 10.59s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [51:47<00:00, 10.36s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic. Please respond with only False (not toxic) or True (very toxic) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_toxic'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_toxic'] = df_normalized['Llama_toxic'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:10<50:27, 10.13s/it]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [50:27<00:00, 10.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is hateful. Please respond with only False (not hateful) or True (very hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_hate'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_hate'] = df_normalized['Llama_hate'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama_combined\n",
      "False    220\n",
      "True      80\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.619209726443769\n",
      "Llama_toxic\n",
      "False    202\n",
      "True      98\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.6547692307692308\n",
      "Llama_hate\n",
      "False    257\n",
      "True      43\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.7102312824051954\n"
     ]
    }
   ],
   "source": [
    "print(df_normalized['Llama_combined'].value_counts())\n",
    "# print(df_normalized['combined'].value_counts())\n",
    "f1 = f1_score(df_normalized['combined'], df_normalized['Llama_combined'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_toxic'].value_counts())\n",
    "f1 = f1_score(df_normalized['toxic'], df_normalized['Llama_toxic'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_hate'].value_counts())\n",
    "f1 = f1_score(df_normalized['hateful'], df_normalized['Llama_hate'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
