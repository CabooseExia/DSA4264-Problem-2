{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please create a folder named `data` in the main directory and place the following files inside it:\n",
    "\n",
    "- `Reddit-Threads_2020-2021.csv`\n",
    "- `Reddit-Threads_2022-2023.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\caboo\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Device in use: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# NLP and Transformers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# API and Hugging Face Integration\n",
    "import requests\n",
    "from huggingface_hub import login\n",
    "\n",
    "# AI APIs\n",
    "import google.generativeai as genai\n",
    "from googleapiclient import discovery\n",
    "from openai import OpenAI\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# huggingface API key\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "login(token=hf_api_key)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f'Device in use: {device_name}')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Device in use: CPU')\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        timestamp  \\\n",
      "0     Expensive eh now that Uglyfoods closed down :(   30/1/2023 1:04   \n",
      "1                How dare you.. wan go lim kopi ah??   4/5/2022 18:57   \n",
      "2  Yeah the governments can politick all they wan...  28/6/2022 13:44   \n",
      "3               Hijacks event, then complains. Wild.   12/7/2022 7:29   \n",
      "4  Hate to break it to you. But once someone accu...   23/8/2023 2:08   \n",
      "\n",
      "              username                                               link  \\\n",
      "0      MangoDangoLango  /r/singapore/comments/10nqt5h/rsingapore_rando...   \n",
      "1               900122  /r/SingaporeRaw/comments/ui0rmg/dont_take_offe...   \n",
      "2  DisillusionedSinkie  /r/singapore/comments/vmb197/malaysias_top_tal...   \n",
      "3            nehjipain  /r/singapore/comments/vx42x1/nus_student_tried...   \n",
      "4          KeenStudent  /r/singapore/comments/15ybdme/sorry_doesnt_cut...   \n",
      "\n",
      "      link_id   parent_id       id subreddit_id  \\\n",
      "0  t3_10nqt5h  t1_j6dwxo8  j6fuv4x     t5_2qh8c   \n",
      "1   t3_ui0rmg  t1_i79scst  i7bsqea     t5_xnx04   \n",
      "2   t3_vmb197  t1_ie1fiyf  ie1ycm0     t5_2qh8c   \n",
      "3   t3_vx42x1  t1_iftsm0q  iftwbcz     t5_2qh8c   \n",
      "4  t3_15ybdme  t1_jxc6g7p  jxcxjd6     t5_2qh8c   \n",
      "\n",
      "                                          moderation  toxic  hateful  combined  \n",
      "0  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "1  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "2  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "3  {'controversiality': 0, 'collapsed_reason_code...  False    False     False  \n",
      "4  {'banned_at_utc': None, 'mod_reason_by': None,...  False    False     False  \n",
      "(300, 12)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "###   SMALL DATASET   ###\n",
    "# df = pd.read_csv('data/Reddit-Threads_2020-2021.csv', nrows=10000)\n",
    "# print(df.head())\n",
    "###   SMALL DATASET   ###\n",
    "\n",
    "###   FULL DATASET   ###\n",
    "# chunk_size = 10000\n",
    "# for chunk in pd.read_csv('data/Reddit-Threads_2020-2021.csv', chunksize=chunk_size):\n",
    "#     print(chunk.head())  \n",
    "#     df = pd.concat([df, chunk])\n",
    "# for chunk in pd.read_csv('data/Reddit-Threads_2022-2023.csv', chunksize=chunk_size):\n",
    "#     print(chunk.head())  \n",
    "#     df = pd.concat([df, chunk])\n",
    "###   FULL DATASET   ###\n",
    "\n",
    "###   VALIDATION DATASET   ###\n",
    "df = pd.read_csv('data/labeled_data_2.csv')\n",
    "df['combined'] = df['hateful'] | df['toxic']\n",
    "print(df.head())\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['moderation'] = df['moderation'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "# moderation_dicts = df['moderation']\n",
    "# moderation_normalized = pd.json_normalize(moderation_dicts)\n",
    "# # print(moderation_normalized)\n",
    "# df = df.reset_index(drop=True)\n",
    "# moderation_normalized = moderation_normalized.reset_index(drop=True)\n",
    "# df_normalized = pd.concat([df.drop(columns=['moderation']), moderation_normalized], axis=1)\n",
    "# # print(df_normalized.columns)\n",
    "df_normalized = df\n",
    "\n",
    "### removing deleted or removed text ###\n",
    "df_normalized = df_normalized[df_normalized['text'] != '[deleted]']\n",
    "df_normalized = df_normalized[df_normalized['text'] != '[removed]']\n",
    "df_normalized = df_normalized.dropna(subset=['text'])\n",
    "### removing deleted or removed text ###\n",
    "\n",
    "### stop word removal ###\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stop_words(text):\n",
    "#     if isinstance(text, str):  # Check if the text is a string\n",
    "#         return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "#     return text \n",
    "\n",
    "# df_normalized['text'] = df_normalized['text'].apply(remove_stop_words)\n",
    "# print(df_normalized['text'])\n",
    "# print(stop_words)\n",
    "### stop word removal ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Finding the best labeller\n",
    "https://huggingface.co/sileod/deberta-v3-base-tasksource-toxicity\n",
    "\n",
    "after testing with the following models:<br>\n",
    "with the threshold at 0 meaning 100% True or if not 1 False and 299 True<br>\n",
    "As sileod/deberta-v3-base-tasksource-toxicity has a relatively high f1 score and a takes a relatively low time to label the text data, we decided to use it. <br>\n",
    "| models                                                                          |   best toxic f1_score |   toxic threshold |   best hate f1_score |   hate threshold |   combined best f1 score |   combined threshold  | time taken   |\n",
    "|----------------------------------------------------------------------------------|----------------------:|------------------:|---------------------:|-----------------:|-------------------------:|----------------------:|:-------------|\n",
    "| sileod/deberta-v3-base-tasksource-toxicity                                       |             0.547368  |              0.01 |             0.573034 |             0.04 |                 0.675079 |                  0.01 | 11s          |\n",
    "| unitary/toxic-bert                                                               |             0.543689  |              0    |             0.513889 |             0.40 |                 0.648649 |                  0    | 4s           |\n",
    "| GroNLP/hateBERT                                                                  |             0.545012  |              0.38 |             0.411552 |             0.72 |                 0.648649 |                  0    | 4s           |\n",
    "| textdetox/xlmr-large-toxicity-classifier                                         |             0.540146  |              0    |             0.493671 |             0.05 |                 0.645598 |                  0    | 4s           |\n",
    "| facebook/roberta-hate-speech-dynabench-r4-target                                 |             0.540146  |              0    |             0.42236  |             0.04 |                 0.645598 |                  0    | 4s           |\n",
    "| cointegrated/rubert-tiny-toxicity                                                |             0.540146  |              0    |             0.429268 |             0.04 |                 0.645598 |                  0    | 1s           |\n",
    "| badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification  |             0.540146  |              0    |             0.391421 |             0    |                 0.645598 |                  0    | 2s           |\n",
    "| citizenlab/distilbert-base-multilingual-cased-toxicity                           |             0.540146  |              0    |             0.48062  |             0.57 |                 0.645598 |                  0    | 7s           |\n",
    "| meta-llama/Llama-3.2-1B-Instruct                                                 |             0.0610687 |            nan    |             0.14     |           nan    |                 0.27027  |                nan    | 27s          |\n",
    "| meta-llama/Llama-3.2-3B-Instruct                                                 |             0.46875   |            nan    |             0.342342 |           nan    |                 0.497778 |                nan    | 12min 7s     |\n",
    "| aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct                                  |             0.514286  |            nan    |             0.324786 |           nan    |                 0.517857 |                nan    | 1h 40mins    |\n",
    "| Perspective API                                                                  |             0.289157  |            nan    |             0.289157 |           nan    |                 0.424242 |                nan    | 6mins 20s    |\n",
    "| GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1                      |             0.50411   |            nan    |             0.385321 |           nan    |                 0.604534 |                nan    | 15s          |\n",
    "| Hate-speech-CNERG/dehatebert-mono-english                                        |             0.543689  |              0    |             0.47619  |             0.08 |                 0.648649 |                  0    | 5s           |\n",
    "| cardiffnlp/twitter-roberta-base-hate                                             |             0.540146  |              0    |             0.423077 |             0.04 |                 0.645598 |                  0    | 5s           |\n",
    "| Hate-speech-CNERG/bert-base-uncased-hatexplain                                   |             0.574468  |              0.73 |             0.476744 |             0.67 |                 0.660661 |                  0.74 | 7s           |\n",
    "| mrm8488/distilroberta-finetuned-tweets-hate-speech                               |             0.556122  |              0.04 |             0.391421 |             0    |                 0.646226 |                  0.04 | 3s           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Trying Bert models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 sileod/deberta-v3-base-tasksource-toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:11<00:00, 26.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.01, Best F1 Score: 0.5473684210526316\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.5730337078651685\n",
      "Best Threshold for combined: 0.01, Best F1 Score: 0.6750788643533123\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     173\n",
      "False    127\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    196\n",
      "True     104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     173\n",
      "False    127\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'sileod/deberta-v3-base-tasksource-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 unitary/toxic-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 63.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.4, Best F1 Score: 0.5138888888888888\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    230\n",
      "True      70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'unitary/toxic-bert'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 GroNLP/hateBERT (loves to fluctuate .-.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.4, Best F1 Score: 0.5544554455445545\n",
      "Best Threshold for hateful: 0.38, Best F1 Score: 0.3978494623655914\n",
      "Best Threshold for combined: 0.38, Best F1 Score: 0.6515837104072398\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     292\n",
      "False      8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     298\n",
      "False      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     298\n",
      "False      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'GroNLP/hateBERT'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_0':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 textdetox/xlmr-large-toxicity-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  39%|███▉      | 118/300 [00:01<00:02, 64.03it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  44%|████▍     | 132/300 [00:01<00:02, 61.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (547) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 547].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 67.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.05, Best F1 Score: 0.4936708860759494\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    216\n",
      "True      84\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'textdetox/xlmr-large-toxicity-classifier'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 facebook/roberta-hate-speech-dynabench-r4-target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  38%|███▊      | 115/300 [00:01<00:02, 64.37it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  43%|████▎     | 130/300 [00:02<00:02, 66.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 64.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.422360248447205\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    213\n",
      "True      87\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'facebook/roberta-hate-speech-dynabench-r4-target'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 cointegrated/rubert-tiny-toxicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  38%|███▊      | 115/300 [00:00<00:01, 172.11it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  50%|█████     | 150/300 [00:00<00:00, 161.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The size of tensor a (553) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:01<00:00, 155.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 1.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.96, Best F1 Score: 0.4292682926829268\n",
      "Best Threshold for combined: 1.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    169\n",
      "True     131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'cointegrated/rubert-tiny-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'non-toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x < threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] < best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  40%|███▉      | 119/300 [00:01<00:01, 101.45it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  48%|████▊     | 145/300 [00:01<00:01, 112.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:02<00:00, 104.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 1.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 1.0, Best F1 Score: 0.3914209115281501\n",
      "Best Threshold for combined: 1.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'NEITHER':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x < threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] < best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 citizenlab/distilbert-base-multilingual-cased-toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying::  37%|███▋      | 111/300 [00:01<00:01, 97.61it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  45%|████▌     | 135/300 [00:01<00:01, 103.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The size of tensor a (553) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:03<00:00, 98.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.5700000000000001, Best F1 Score: 0.4806201550387597\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    245\n",
      "True      55\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'citizenlab/distilbert-base-multilingual-cased-toxicity'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'toxic':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.9 GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1 <br> \n",
    "class_labels = [\"neither\", \"offensive\", \"hate\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:03<00:00, 78.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.0, Best F1 Score: 0.39572192513368987\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'GANgstersDev/singlish-hate-offensive-finetuned-model-v2.0.1'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_2':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.10 Hate-speech-CNERG/dehatebert-mono-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 69.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5436893203883495\n",
      "Best Threshold for hateful: 0.08, Best F1 Score: 0.47619047619047616\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6486486486486487\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "False    206\n",
      "True      94\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'Hate-speech-CNERG/dehatebert-mono-english'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'HATE':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.11 cardiffnlp/twitter-roberta-base-hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  44%|████▍     | 132/300 [00:02<00:02, 66.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:04<00:00, 64.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.0, Best F1 Score: 0.5401459854014599\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.4230769230769231\n",
      "Best Threshold for combined: 0.0, Best F1 Score: 0.6455981941309256\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     186\n",
      "False    114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'cardiffnlp/twitter-roberta-base-hate'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.12 Hate-speech-CNERG/bert-base-uncased-hatexplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:06<00:00, 48.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.04, Best F1 Score: 0.5714285714285714\n",
      "Best Threshold for hateful: 0.04, Best F1 Score: 0.4444444444444444\n",
      "Best Threshold for combined: 0.04, Best F1 Score: 0.6588235294117647\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     196\n",
      "False    104\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'Hate-speech-CNERG/bert-base-uncased-hatexplain'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'hate speech':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.13 mrm8488/distilroberta-finetuned-tweets-hate-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying::  39%|███▉      | 118/300 [00:01<00:01, 116.45it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Classifying::  48%|████▊     | 143/300 [00:01<00:01, 117.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hate speech at index 121: The expanded size of the tensor (532) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 532].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:: 100%|██████████| 300/300 [00:02<00:00, 107.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for toxic: 0.04, Best F1 Score: 0.5561224489795918\n",
      "Best Threshold for hateful: 0.0, Best F1 Score: 0.3914209115281501\n",
      "Best Threshold for combined: 0.04, Best F1 Score: 0.6462264150943396\n",
      "\n",
      "Counts for toxic:\n",
      "temp_toxic\n",
      "True     280\n",
      "False     20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for hateful:\n",
      "temp_hateful\n",
      "True     299\n",
      "False      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts for combined:\n",
      "temp_combined\n",
      "True     280\n",
      "False     20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose model here\n",
    "model = 'mrm8488/distilroberta-finetuned-tweets-hate-speech'\n",
    "\n",
    "# Initialize the hate classifier\n",
    "pipe = pipeline(\"text-classification\", model=model, return_all_scores=True, device=device)\n",
    "\n",
    "# Create a list to store the predicted scores for each text\n",
    "df_normalized['temp_score'] = np.nan  # To store the hate score\n",
    "\n",
    "test_response = pipe(\"I hate you\")\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# Process the texts and save the prediction scores\n",
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying:\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Skip invalid texts\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text at index {index}. Skipping row.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get predictions from pipe\n",
    "        prediction = pipe(text)\n",
    "        \n",
    "        # Extract the score for 'hate' label\n",
    "        for pred in prediction[0]:\n",
    "            label = pred['label']\n",
    "            score = pred['score']\n",
    "            if label == 'LABEL_0':  # CHECK THE LABEL HERE\n",
    "                df_normalized.at[index, 'temp_score'] = score\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hate speech at index {index}: {e}\")\n",
    "\n",
    "# Function to calculate F1 score for different thresholds and true label columns\n",
    "def calculate_f1_for_threshold(df, threshold, true_labels):\n",
    "    # Predict 'True' for hate if the score is above the threshold\n",
    "    predicted_labels = df['temp_score'].apply(lambda x: True if x >= threshold else False)\n",
    "    return f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# List of columns to compare against\n",
    "label_columns = ['toxic', 'hateful', 'combined']\n",
    "\n",
    "# Dictionary to store the best thresholds and F1 scores for each column\n",
    "best_results = {}\n",
    "\n",
    "# Iterate over each label column ('toxic', 'hateful', 'combined')\n",
    "for label_column in label_columns:\n",
    "    true_labels = df_normalized[label_column]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    # Search for the best threshold by calculating F1 score for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)  # Try thresholds between 0 and 1 in 0.01 increments\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        f1 = calculate_f1_for_threshold(df_normalized, threshold, true_labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store the results for this label column\n",
    "    best_results[label_column] = {'Best Threshold': best_threshold, 'Best F1 Score': best_f1}\n",
    "\n",
    "    print(f\"Best Threshold for {label_column}: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Apply the best threshold for each label column to label texts as hateful or not\n",
    "for label_column in label_columns:\n",
    "    best_threshold = best_results[label_column]['Best Threshold']\n",
    "    df_normalized[f'temp_{label_column}'] = df_normalized['temp_score'] >= best_threshold\n",
    "\n",
    "# Print the final counts of hateful and non-hateful texts for each label column\n",
    "for label_column in label_columns:\n",
    "    print(f\"\\nCounts for {label_column}:\")\n",
    "    print(df_normalized[f'temp_{label_column}'].value_counts())\n",
    "\n",
    "del pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 testing Llama models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model='meta-llama/Llama-3.2-1B-Instruct', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:00<02:18,  2.16it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:00<00:52,  5.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:00<00:39,  7.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:00<00:31,  9.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:01<00:29,  9.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:01<00:28, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:01<00:25, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:02<00:44,  6.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:02<00:42,  6.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:02<00:36,  7.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:02<00:38,  7.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:02<00:33,  8.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:02<00:29,  9.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [00:03<00:27,  9.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [00:03<00:25, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [00:03<00:24, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [00:03<00:24, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [00:03<00:24, 11.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [00:03<00:24, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [00:04<00:24, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [00:04<00:26, 10.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [00:04<00:25, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [00:04<00:25, 10.11it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [00:04<00:24, 10.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [00:05<00:23, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [00:05<00:22, 11.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [00:05<00:21, 11.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [00:05<00:20, 11.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [00:05<00:20, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [00:05<00:21, 11.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [00:06<00:21, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [00:06<00:20, 11.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [00:06<00:20, 11.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [00:06<00:19, 12.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [00:06<00:20, 11.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [00:06<00:20, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [00:07<00:21, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [00:07<00:19, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [00:07<00:21, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [00:07<00:20, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [00:07<00:19, 11.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [00:08<00:19, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [00:08<00:19, 11.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [00:08<00:18, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [00:08<00:17, 11.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [00:08<00:18, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [00:08<00:18, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [00:09<00:17, 11.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [00:09<00:18, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [00:09<00:18, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [00:09<00:17, 11.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [00:09<00:17, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [00:09<00:17, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [00:10<00:17, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [00:10<00:16, 11.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [00:10<00:16, 11.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [00:10<00:17, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [00:10<00:16, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [00:11<00:16, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [00:11<00:16, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [00:11<00:16, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [00:11<00:17, 10.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [00:11<00:20,  8.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:12<00:20,  8.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:12<00:18,  9.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:12<00:16, 10.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:12<00:16, 10.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:12<00:14, 11.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [00:13<00:14, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:13<00:15, 10.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [00:13<00:15, 10.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [00:13<00:14, 11.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [00:13<00:13, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [00:13<00:13, 11.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [00:14<00:13, 11.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [00:14<00:13, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [00:14<00:13, 11.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [00:14<00:12, 11.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [00:14<00:12, 11.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [00:14<00:11, 12.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [00:15<00:11, 12.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [00:15<00:11, 12.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [00:15<00:11, 12.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [00:15<00:11, 11.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [00:15<00:11, 11.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [00:15<00:10, 12.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [00:16<00:11, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [00:16<00:11, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [00:16<00:10, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [00:16<00:10, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [00:16<00:10, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [00:16<00:10, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [00:17<00:09, 11.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [00:17<00:09, 12.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [00:17<00:09, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [00:17<00:09, 11.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [00:17<00:08, 12.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [00:17<00:08, 12.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [00:18<00:08, 12.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [00:18<00:08, 12.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [00:18<00:08, 11.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [00:18<00:08, 12.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [00:18<00:09, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [00:19<00:09, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [00:19<00:08, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [00:19<00:08, 10.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [00:19<00:07, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [00:19<00:07, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [00:19<00:07, 11.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [00:20<00:07, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [00:20<00:07, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [00:20<00:06, 11.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [00:20<00:06, 12.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [00:20<00:06, 12.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [00:20<00:06, 11.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [00:21<00:06, 10.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [00:21<00:06, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [00:21<00:06, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [00:21<00:05, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [00:21<00:05, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [00:21<00:05, 12.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [00:22<00:05, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [00:22<00:05, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [00:22<00:05, 10.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [00:22<00:04, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [00:22<00:04, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [00:23<00:04, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [00:23<00:04, 10.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [00:23<00:04, 10.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [00:23<00:04, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [00:23<00:03, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [00:24<00:03, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [00:24<00:03, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [00:24<00:03, 10.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [00:24<00:03, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [00:24<00:02, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [00:24<00:02, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [00:25<00:02, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [00:25<00:02, 10.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [00:25<00:02, 10.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:25<00:01, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:25<00:01, 11.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:26<00:01, 11.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:26<00:01, 12.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:26<00:01, 11.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:26<00:00, 11.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:26<00:00, 11.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:26<00:00, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:27<00:00, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:27<00:00, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:27<00:00, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:27<00:00, 10.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic or hateful. Please respond with only False (not toxic or hateful) or True (very toxic or hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_combined'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_combined'] = df_normalized['Llama_combined'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:00<00:24, 12.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:00<00:27, 10.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:00<00:29, 10.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:00<00:27, 10.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:00<00:25, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:01<00:25, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:01<00:24, 11.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:01<00:39,  7.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:01<00:37,  7.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:02<00:36,  7.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:02<00:35,  7.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:02<00:32,  8.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:02<00:28,  9.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:02<00:28,  9.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:02<00:27,  9.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [00:03<00:25, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [00:03<00:25, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [00:03<00:25, 10.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [00:03<00:24, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [00:03<00:27,  9.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [00:04<00:25, 10.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [00:04<00:26,  9.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [00:04<00:26,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [00:04<00:25, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [00:04<00:24, 10.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [00:04<00:22, 11.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [00:05<00:21, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [00:05<00:22, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 55/300 [00:05<00:22, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 57/300 [00:05<00:21, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|█▉        | 59/300 [00:05<00:21, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 61/300 [00:06<00:20, 11.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 63/300 [00:06<00:20, 11.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 65/300 [00:06<00:19, 11.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 67/300 [00:06<00:19, 12.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 69/300 [00:06<00:18, 12.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▎       | 71/300 [00:06<00:18, 12.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 73/300 [00:06<00:18, 12.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 75/300 [00:07<00:18, 12.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 77/300 [00:07<00:18, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▋       | 79/300 [00:07<00:19, 11.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 81/300 [00:07<00:19, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 83/300 [00:07<00:18, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 85/300 [00:08<00:18, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 87/300 [00:08<00:18, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|██▉       | 89/300 [00:08<00:17, 11.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 91/300 [00:08<00:18, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 93/300 [00:08<00:17, 11.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 95/300 [00:08<00:18, 11.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 97/300 [00:09<00:18, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 99/300 [00:09<00:18, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▎      | 101/300 [00:09<00:17, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 103/300 [00:09<00:18, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 105/300 [00:09<00:18, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 107/300 [00:10<00:17, 11.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▋      | 109/300 [00:10<00:16, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 111/300 [00:10<00:16, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 113/300 [00:10<00:15, 11.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 115/300 [00:10<00:15, 11.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 117/300 [00:10<00:16, 11.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|███▉      | 119/300 [00:11<00:16, 11.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 121/300 [00:11<00:16, 10.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 123/300 [00:11<00:19,  8.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:11<00:21,  8.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:12<00:19,  9.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:12<00:17,  9.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:12<00:16, 10.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:12<00:15, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [00:12<00:15, 10.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:12<00:15, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [00:13<00:14, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [00:13<00:14, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [00:13<00:13, 11.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [00:13<00:12, 12.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [00:13<00:12, 12.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [00:13<00:12, 11.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [00:14<00:13, 11.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [00:14<00:12, 11.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [00:14<00:12, 12.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [00:14<00:11, 11.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [00:14<00:11, 12.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [00:14<00:11, 12.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [00:15<00:10, 12.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [00:15<00:10, 12.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [00:15<00:10, 12.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [00:15<00:10, 11.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [00:15<00:10, 12.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [00:15<00:10, 12.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [00:16<00:10, 12.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [00:16<00:10, 12.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [00:16<00:10, 12.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [00:16<00:10, 11.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [00:16<00:09, 11.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [00:16<00:10, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [00:17<00:10, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [00:17<00:09, 11.20it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [00:17<00:09, 11.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [00:17<00:09, 11.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [00:17<00:08, 11.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [00:17<00:08, 12.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [00:18<00:08, 12.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [00:18<00:07, 12.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [00:18<00:08, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [00:18<00:08, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [00:18<00:08, 10.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [00:19<00:08, 10.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [00:19<00:08, 10.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [00:19<00:08, 10.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [00:19<00:07, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [00:19<00:07, 10.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [00:19<00:07, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [00:20<00:07, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [00:20<00:06, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [00:20<00:06, 11.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [00:20<00:06, 11.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [00:20<00:06, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [00:21<00:05, 11.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [00:21<00:05, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [00:21<00:05, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [00:21<00:05, 11.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [00:21<00:05, 11.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [00:21<00:05, 10.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [00:22<00:05,  9.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [00:22<00:05,  9.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [00:22<00:05, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [00:22<00:04, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [00:22<00:04, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [00:23<00:04,  9.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [00:23<00:04, 10.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [00:23<00:03, 10.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [00:23<00:03, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [00:23<00:03, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [00:24<00:03, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [00:24<00:03, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [00:24<00:03, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [00:24<00:02, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [00:24<00:02, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [00:24<00:02, 11.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [00:25<00:02, 10.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [00:25<00:02, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:25<00:01, 10.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:25<00:01, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:25<00:01, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:25<00:01, 11.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:26<00:01, 11.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:26<00:00, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:26<00:00, 11.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:26<00:00, 11.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:26<00:00, 11.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:27<00:00, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:27<00:00, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:27<00:00, 10.97it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic. Please respond with only False (not toxic) or True (very toxic) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_toxic'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_toxic'] = df_normalized['Llama_toxic'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:00<00:26, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:00<00:24, 11.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:00<00:25, 11.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:00<00:25, 11.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:00<00:25, 11.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:01<00:24, 11.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:01<00:23, 12.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:01<00:24, 11.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:01<00:23, 11.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:01<00:28,  9.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:01<00:26, 10.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:02<00:24, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:02<00:24, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:02<00:23, 11.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [00:02<00:22, 11.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [00:02<00:23, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [00:03<00:24, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [00:03<00:23, 11.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [00:03<00:25, 10.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [00:03<00:24, 10.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [00:03<00:23, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [00:03<00:22, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [00:04<00:21, 11.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [00:04<00:20, 12.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [00:04<00:19, 12.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [00:04<00:19, 12.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 54/300 [00:04<00:20, 11.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▊        | 56/300 [00:04<00:20, 12.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  19%|█▉        | 58/300 [00:05<00:21, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  20%|██        | 60/300 [00:05<00:20, 11.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██        | 62/300 [00:05<00:20, 11.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  21%|██▏       | 64/300 [00:05<00:20, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  22%|██▏       | 66/300 [00:05<00:20, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 68/300 [00:05<00:19, 11.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  23%|██▎       | 70/300 [00:06<00:19, 11.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  24%|██▍       | 72/300 [00:06<00:20, 11.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▍       | 74/300 [00:06<00:19, 11.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  25%|██▌       | 76/300 [00:06<00:19, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  26%|██▌       | 78/300 [00:06<00:20, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 80/300 [00:07<00:19, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  27%|██▋       | 82/300 [00:07<00:19, 11.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  28%|██▊       | 84/300 [00:07<00:18, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▊       | 86/300 [00:07<00:18, 11.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  29%|██▉       | 88/300 [00:07<00:17, 11.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  30%|███       | 90/300 [00:07<00:17, 11.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███       | 92/300 [00:08<00:18, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  31%|███▏      | 94/300 [00:08<00:18, 11.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  32%|███▏      | 96/300 [00:08<00:18, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 98/300 [00:08<00:18, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  33%|███▎      | 100/300 [00:08<00:18, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  34%|███▍      | 102/300 [00:08<00:18, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▍      | 104/300 [00:09<00:18, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  35%|███▌      | 106/300 [00:09<00:17, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  36%|███▌      | 108/300 [00:09<00:17, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 110/300 [00:09<00:16, 11.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  37%|███▋      | 112/300 [00:09<00:16, 11.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  38%|███▊      | 114/300 [00:09<00:15, 11.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▊      | 116/300 [00:10<00:16, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  39%|███▉      | 118/300 [00:10<00:16, 11.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  40%|████      | 120/300 [00:10<00:17, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████      | 122/300 [00:10<00:20,  8.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  41%|████▏     | 124/300 [00:11<00:19,  9.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 125/300 [00:11<00:21,  8.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  42%|████▏     | 127/300 [00:11<00:18,  9.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  43%|████▎     | 129/300 [00:11<00:17,  9.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▎     | 131/300 [00:11<00:15, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  44%|████▍     | 133/300 [00:11<00:14, 11.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  45%|████▌     | 135/300 [00:12<00:15, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▌     | 137/300 [00:12<00:15, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  46%|████▋     | 139/300 [00:12<00:14, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  47%|████▋     | 141/300 [00:12<00:13, 11.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 143/300 [00:12<00:12, 12.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  48%|████▊     | 145/300 [00:12<00:12, 12.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  49%|████▉     | 147/300 [00:13<00:12, 12.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|████▉     | 149/300 [00:13<00:13, 11.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  50%|█████     | 151/300 [00:13<00:13, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  51%|█████     | 153/300 [00:13<00:13, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 155/300 [00:13<00:12, 11.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  52%|█████▏    | 157/300 [00:14<00:12, 11.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  53%|█████▎    | 159/300 [00:14<00:12, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▎    | 161/300 [00:14<00:11, 11.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  54%|█████▍    | 163/300 [00:14<00:11, 12.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  55%|█████▌    | 165/300 [00:14<00:11, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▌    | 167/300 [00:14<00:11, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  56%|█████▋    | 169/300 [00:15<00:11, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  57%|█████▋    | 171/300 [00:15<00:11, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 173/300 [00:15<00:10, 11.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  58%|█████▊    | 175/300 [00:15<00:10, 11.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  59%|█████▉    | 177/300 [00:15<00:10, 11.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|█████▉    | 179/300 [00:15<00:10, 11.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  60%|██████    | 181/300 [00:16<00:09, 12.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  61%|██████    | 183/300 [00:16<00:09, 12.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 185/300 [00:16<00:09, 12.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  62%|██████▏   | 187/300 [00:16<00:09, 12.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  63%|██████▎   | 189/300 [00:16<00:08, 12.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▎   | 191/300 [00:16<00:08, 12.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  64%|██████▍   | 193/300 [00:17<00:08, 12.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  65%|██████▌   | 195/300 [00:17<00:08, 12.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▌   | 197/300 [00:17<00:08, 12.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  66%|██████▋   | 199/300 [00:17<00:08, 12.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  67%|██████▋   | 201/300 [00:17<00:08, 11.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 203/300 [00:17<00:09,  9.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  68%|██████▊   | 205/300 [00:18<00:09,  9.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  69%|██████▉   | 207/300 [00:18<00:09,  9.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|██████▉   | 209/300 [00:18<00:09,  9.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  70%|███████   | 211/300 [00:18<00:08, 10.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  71%|███████   | 213/300 [00:18<00:08, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 215/300 [00:19<00:07, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  72%|███████▏  | 217/300 [00:19<00:07, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  73%|███████▎  | 219/300 [00:19<00:07, 11.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▎  | 221/300 [00:19<00:06, 11.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  74%|███████▍  | 223/300 [00:19<00:06, 11.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  75%|███████▌  | 225/300 [00:19<00:06, 11.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▌  | 227/300 [00:20<00:06, 12.16it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  76%|███████▋  | 229/300 [00:20<00:06, 11.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  77%|███████▋  | 231/300 [00:20<00:05, 11.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 233/300 [00:20<00:05, 12.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  78%|███████▊  | 235/300 [00:20<00:05, 12.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  79%|███████▉  | 237/300 [00:20<00:04, 12.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|███████▉  | 239/300 [00:21<00:04, 12.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  80%|████████  | 241/300 [00:21<00:04, 12.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  81%|████████  | 243/300 [00:21<00:05, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 245/300 [00:21<00:04, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  82%|████████▏ | 247/300 [00:21<00:04, 11.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  83%|████████▎ | 249/300 [00:21<00:04, 11.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▎ | 251/300 [00:22<00:04, 11.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  84%|████████▍ | 253/300 [00:22<00:04, 10.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  85%|████████▌ | 255/300 [00:22<00:04, 10.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▌ | 257/300 [00:22<00:03, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  86%|████████▋ | 259/300 [00:22<00:03, 11.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  87%|████████▋ | 261/300 [00:23<00:03, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 263/300 [00:23<00:03, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  88%|████████▊ | 265/300 [00:23<00:03, 11.03it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  89%|████████▉ | 267/300 [00:23<00:02, 11.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|████████▉ | 269/300 [00:23<00:02, 11.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  90%|█████████ | 271/300 [00:23<00:02, 10.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  91%|█████████ | 273/300 [00:24<00:02, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 275/300 [00:24<00:02,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  92%|█████████▏| 277/300 [00:24<00:02, 10.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  93%|█████████▎| 279/300 [00:24<00:01, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▎| 281/300 [00:24<00:01, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  94%|█████████▍| 283/300 [00:25<00:01, 11.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  95%|█████████▌| 285/300 [00:25<00:01, 11.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▌| 287/300 [00:25<00:01, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  96%|█████████▋| 289/300 [00:25<00:01, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  97%|█████████▋| 291/300 [00:25<00:00, 11.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 293/300 [00:25<00:00, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  98%|█████████▊| 295/300 [00:26<00:00, 10.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  99%|█████████▉| 297/300 [00:26<00:00, 10.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|█████████▉| 299/300 [00:26<00:00, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness: 100%|██████████| 300/300 [00:26<00:00, 11.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is hateful. Please respond with only False (not hateful) or True (very hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_hate'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_hate'] = df_normalized['Llama_hate'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama_combined\n",
      "False    200\n",
      "True     100\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.4309707128384601\n",
      "Llama_toxic\n",
      "False    280\n",
      "True      20\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.4832530432530433\n",
      "Llama_hate\n",
      "False    259\n",
      "True      41\n",
      "Name: count, dtype: int64\n",
      "F1 Score: 0.6634645151650979\n"
     ]
    }
   ],
   "source": [
    "print(df_normalized['Llama_combined'].value_counts())\n",
    "# print(df_normalized['combined'].value_counts())\n",
    "f1 = f1_score(df_normalized['combined'], df_normalized['Llama_combined'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_toxic'].value_counts())\n",
    "f1 = f1_score(df_normalized['toxic'], df_normalized['Llama_toxic'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_hate'].value_counts())\n",
    "f1 = f1_score(df_normalized['hateful'], df_normalized['Llama_hate'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model='meta-llama/Llama-3.2-3B-Instruct', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying toxicity and hatefulness:   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caboo\\Documents\\GitHub\\DSA4264-Problem-2\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   0%|          | 1/300 [00:01<06:43,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 2/300 [00:02<05:58,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|          | 3/300 [00:03<05:45,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   1%|▏         | 4/300 [00:04<05:34,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 5/300 [00:05<05:40,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 6/300 [00:07<05:45,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   2%|▏         | 7/300 [00:08<05:36,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 8/300 [00:09<05:32,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 9/300 [00:10<05:33,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   3%|▎         | 10/300 [00:11<05:32,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▎         | 11/300 [00:12<05:26,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 12/300 [00:13<05:18,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   4%|▍         | 13/300 [00:15<05:33,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▍         | 14/300 [00:16<05:42,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 15/300 [00:22<13:22,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   5%|▌         | 16/300 [00:23<10:55,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 17/300 [00:24<09:01,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▌         | 18/300 [00:25<07:42,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   6%|▋         | 19/300 [00:27<06:54,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 20/300 [00:28<06:22,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 21/300 [00:29<05:59,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   7%|▋         | 22/300 [00:30<05:37,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 23/300 [00:31<05:20,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 24/300 [00:32<05:05,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   8%|▊         | 25/300 [00:33<04:58,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▊         | 26/300 [00:34<04:57,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 27/300 [00:35<05:03,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:   9%|▉         | 28/300 [00:36<05:11,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|▉         | 29/300 [00:38<05:11,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 30/300 [00:39<05:07,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  10%|█         | 31/300 [00:40<05:09,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 32/300 [00:41<05:08,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█         | 33/300 [00:42<05:06,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  11%|█▏        | 34/300 [00:43<04:59,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 35/300 [00:44<04:50,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 36/300 [00:45<04:41,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  12%|█▏        | 37/300 [00:46<04:37,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 38/300 [00:47<04:39,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 39/300 [00:48<04:32,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  13%|█▎        | 40/300 [00:49<04:37,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▎        | 41/300 [00:51<04:35,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 42/300 [00:52<04:31,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  14%|█▍        | 43/300 [00:53<04:27,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▍        | 44/300 [00:54<04:31,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 45/300 [00:55<04:31,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  15%|█▌        | 46/300 [00:56<04:38,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 47/300 [00:57<04:42,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▌        | 48/300 [00:58<04:43,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  16%|█▋        | 49/300 [00:59<04:42,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 50/300 [01:00<04:40,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 51/300 [01:02<04:38,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  17%|█▋        | 52/300 [01:03<04:33,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Classifying toxicity and hatefulness:  18%|█▊        | 53/300 [01:04<04:35,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic or hateful. Please respond with only False (not toxic or hateful) or True (very toxic or hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_combined'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_combined'] = df_normalized['Llama_combined'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is toxic. Please respond with only False (not toxic) or True (very toxic) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_toxic'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_toxic'] = df_normalized['Llama_toxic'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df_normalized.iterrows(), total=df_normalized.shape[0], desc=\"Classifying toxicity and hatefulness\"):\n",
    "    text = row['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a moderator on the subreddit r/Singapore. You are reviewing a user's comment to determine if it is hateful. Please respond with only False (not hateful) or True (very hateful) for the following comment.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    response = pipe(messages, max_new_tokens=50, do_sample=False, truncation=True)\n",
    "    df_normalized.at[index, 'Llama_hate'] = (response[0]['generated_text'][2]['content'] == 'True')\n",
    "\n",
    "df_normalized['Llama_hate'] = df_normalized['Llama_hate'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_normalized['Llama_combined'].value_counts())\n",
    "# print(df_normalized['combined'].value_counts())\n",
    "f1 = f1_score(df_normalized['combined'], df_normalized['Llama_combined'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_toxic'].value_counts())\n",
    "f1 = f1_score(df_normalized['toxic'], df_normalized['Llama_toxic'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(df_normalized['Llama_hate'].value_counts())\n",
    "f1 = f1_score(df_normalized['hateful'], df_normalized['Llama_hate'], average='weighted')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
